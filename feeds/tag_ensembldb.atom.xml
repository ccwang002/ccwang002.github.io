<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Liang-Bo Wang's Blog - ensembldb</title><link href="https://blog.liang2.tw/" rel="alternate"></link><link href="https://blog.liang2.tw/feeds/tag_ensembldb.atom.xml" rel="self"></link><id>https://blog.liang2.tw/</id><updated>2022-10-05T18:47:48-05:00</updated><entry><title>Use DuckDB in ensembldb to query Ensembl's genome annotations</title><link href="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/" rel="alternate"></link><published>2022-10-05T00:00:00-05:00</published><updated>2022-10-05T18:47:48-05:00</updated><author><name>Liang-Bo Wang</name></author><id>tag:blog.liang2.tw,2022-10-05:/posts/2022/10/use-duckdb-in-ensembldb/</id><summary type="html">&lt;p&gt;I have been using ensembldb to query genome annotations locally, which stores the Ensembl annotations in a offline SQLite database. By replacing the database engine with DuckDB, genome-wide queries are faster with small impact on gene specific queries (depending on the usage). DuckDB database&amp;rsquo;s file size is also smaller, and it can be even smaller by offloading the tables to external Parquet files.&lt;/p&gt;</summary><content type="html">&lt;!-- cSpell:words sexchrom OLAP Hsapiens pyarrow ensdb zstandard zstd --&gt;

&lt;p&gt;To query genome annotations locally, &lt;a href="https://bioconductor.org/packages/release/bioc/html/ensembldb.html"&gt;ensembldb&lt;/a&gt; has been my go-to approach.
While I&amp;rsquo;ve already said many good things about this R package (&lt;a href="https://blog.liang2.tw/posts/2016/05/biocondutor-ensembl-reference/"&gt;1&lt;/a&gt;, &lt;a href="https://blog.liang2.tw/posts/2017/11/use-ensdb-database-in-python/"&gt;2&lt;/a&gt;, &lt;a href="https://blog.liang2.tw/posts/2019/01/build-ensdb-from-local-mysql/"&gt;3&lt;/a&gt;), here&amp;rsquo;s a summary of my favorite features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I can use the same Ensembl version throughout my project (as a SQLite database)&lt;/li&gt;
&lt;li&gt;I can query the genome-wide annotations and their locations easily and offline&lt;/li&gt;
&lt;li&gt;Nice integration to R&amp;rsquo;s ecosystem that I can easily combine the extracted annotations with my data and other annotations using &lt;a href="https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html"&gt;GenomicRanges&lt;/a&gt; and &lt;a href="https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html"&gt;SummarizedExperiment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Language agnostic to use its database (say, &lt;a href="https://blog.liang2.tw/posts/2017/11/use-ensdb-database-in-python/"&gt;I can query the same db in Python&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt; is designed for analytical query workloads (aka &lt;a href="https://en.wikipedia.org/wiki/Online_analytical_processing"&gt;OLAP&lt;/a&gt;), I decided to convert ensembldb&amp;rsquo;s SQLite database to DuckDB and try it in some of my common analysis scenarios.
DuckDB has a similar look-and-feel to SQLite.
Also, it uses a columnar storage and supports query into external &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; tables.
I tried out some of these user-friendly features in this exercise.&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#convert-ensembldbs-database-to-duckdb-through-parquet"&gt;Convert ensembldb&amp;rsquo;s database to DuckDB through Parquet&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#load-parquet-tables-to-duckdb"&gt;Load Parquet tables to DuckDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#database-file-size-comparison"&gt;Database file size comparison&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#use-duckdb-in-ensembldb"&gt;Use DuckDB in ensembldb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benchmark-the-databases"&gt;Benchmark the databases&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#genome-wide-annotation-query"&gt;Genome-wide annotation query&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#another-genome-wide-annotation-query"&gt;Another genome-wide annotation query&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#gene-specific-lookup"&gt;Gene-specific lookup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id="convert-ensembldbs-database-to-duckdb-through-parquet"&gt;Convert ensembldb&amp;rsquo;s database to DuckDB through Parquet&lt;/h2&gt;
&lt;p&gt;The first step is to convert ensembldb&amp;rsquo;s SQLite database to DuckDB&lt;sup id="fnref:sqlite-to-duckdb"&gt;&lt;a class="footnote-ref" href="#fn:sqlite-to-duckdb"&gt;1&lt;/a&gt;&lt;/sup&gt;.
I decided to export the SQLite tables as individual Parquet files, and then reload them back to DuckDB.
So we could also test the DuckDB&amp;rsquo;s ability to query external parquet files directly.&lt;/p&gt;
&lt;p&gt;For this exercise, I used the &lt;a href="https://www.ensembl.org/Homo_sapiens/"&gt;latest Ensembl release&lt;/a&gt; (v107).
We can download the corresponding SQLite database from &lt;a href="https://annotationhub.bioconductor.org/package2/AHEnsDbs"&gt;AnnotationHub&amp;rsquo;s web interface&lt;/a&gt; (its object ID is &lt;a href="https://annotationhub.bioconductor.org/ahid/AH104864"&gt;AH104864&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl -Lo EnsDb.Hsapiens.v107.sqlite &lt;span class="se"&gt;\&lt;/span&gt;
    https://annotationhub.bioconductor.org/fetch/111610
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We use &lt;a href="https://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt; to fetch the schema of all the tables:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sqlalchemy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MetaData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;

&lt;span class="n"&gt;engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sqlite:///EnsDb.Hsapiens.v107.sqlite&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;metadata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MetaData&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reflect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can then list all the tables and their column data types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;db_tables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sorted_tables&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;db_tables&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;, &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; (&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;)&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# chromosome: seq_name (TEXT), seq_length (INTEGER), is_circular (INTEGER)&lt;/span&gt;
&lt;span class="c1"&gt;# gene: gene_id (TEXT), gene_name (TEXT), gene_biotype (TEXT),&lt;/span&gt;
&lt;span class="c1"&gt;#   gene_seq_start (INTEGER), gene_seq_end (INTEGER),&lt;/span&gt;
&lt;span class="c1"&gt;#   seq_name (TEXT), seq_strand (INTEGER),...&lt;/span&gt;
&lt;span class="c1"&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the correct data type mapping, we can export all the tables as Parquet by &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt; and &lt;a href="https://arrow.apache.org/docs/python/index.html"&gt;PyArrow&lt;/a&gt;.
Since there are quite many text columns, I also used &lt;a href="https://facebook.github.io/zstd/"&gt;zstandard&lt;/a&gt; to compress the Parquet (with a higher compression level):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pyarrow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pa&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pyarrow.parquet&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pq&lt;/span&gt;

&lt;span class="n"&gt;sqlite_to_pyarrow_type_mapping&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;TEXT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;INTEGER&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;REAL&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# Read each SQLite table as a Arrow table&lt;/span&gt;
&lt;span class="n"&gt;arrow_tables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;metadata&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sorted_tables&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Construct the corresponding pyarrow schema&lt;/span&gt;
        &lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sqlite_to_pyarrow_type_mapping&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;
        &lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;arrow_tables&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pa&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pandas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_sql_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;coerce_float&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;preserve_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Write each Arrow table to a zstd compressed Parquet&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;table_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arrow_tables&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;pq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ensdb_v107/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;table_name&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;.parquet&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;compression&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;zstd&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;compression_level&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="load-parquet-tables-to-duckdb"&gt;Load Parquet tables to DuckDB&lt;/h3&gt;
&lt;p&gt;Finally, we can load the exported Parquet tables to DuckDB.
Here I tested a few approaches:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create views to the external Parquet files (no content loaded to the db)&lt;/li&gt;
&lt;li&gt;Load the full content&lt;/li&gt;
&lt;li&gt;Load the full content and index the tables (same as the original SQLite db)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since DuckDB has native support for Parquet files, the syntax is straightforward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;-- Install and activate the extension&lt;/span&gt;
&lt;span class="n"&gt;INSTALL&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;parquet&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;LOAD&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;parquet&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c1"&gt;-- To create views to external Parquet&lt;/span&gt;
&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;VIEW&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;table&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;AS&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;./ensdb_v107/&amp;lt;table&amp;gt;.parquet&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c1"&gt;-- To load the full content from external Parquet&lt;/span&gt;
&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;TABLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;table&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;AS&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;./ensdb_v107/&amp;lt;table&amp;gt;.parquet&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="c1"&gt;-- To index the table (use .schema to get the original index definitions)&lt;/span&gt;
&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;UNIQUE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;INDEX&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene_gene_id_idx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_id&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;INDEX&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene_gene_name_idx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_name&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;INDEX&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene_seq_name_idx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seq_name&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that I didn&amp;rsquo;t try to &amp;ldquo;optimize&amp;rdquo; the table indices for my queries.
I simply mirrored the same index definition from the original SQLite database.&lt;/p&gt;
&lt;p&gt;DuckDB&amp;rsquo;s commandline interface works like SQLite.
And it keeps the database in a single file too.
The full conversion including the Parquet step took about 10 seconds to complete.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;duckdb -echo ensdb_v107.duckdb &amp;lt; create_duckdb.sql
duckdb -readonly ensdb_v107.duckdb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="database-file-size-comparison"&gt;Database file size comparison&lt;/h3&gt;
&lt;p&gt;Here shows the file size of the databases created with different settings:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Database&lt;/th&gt;
&lt;th style="text-align: right;"&gt;File size&lt;/th&gt;
&lt;th style="text-align: right;"&gt;(%)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;SQLite no indexed&lt;/td&gt;
&lt;td style="text-align: right;"&gt;243MB&lt;/td&gt;
&lt;td style="text-align: right;"&gt;57.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;SQLite (original)&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;strong&gt;420MB&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;strong&gt;100.0&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;DuckDB with external Parquets&lt;/td&gt;
&lt;td style="text-align: right;"&gt;37.6MB&lt;/td&gt;
&lt;td style="text-align: right;"&gt;9.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;DuckDB&lt;/td&gt;
&lt;td style="text-align: right;"&gt;169MB&lt;/td&gt;
&lt;td style="text-align: right;"&gt;40.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;DuckDB indexed&lt;/td&gt;
&lt;td style="text-align: right;"&gt;528MB&lt;/td&gt;
&lt;td style="text-align: right;"&gt;125.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;DuckDB with external Parquets yields the smallest file (~9% of the original size).
It&amp;rsquo;s probably due to a lot of text columns in the database, and zstd compression works really well for the plain text.
This approach could make the ensembldb database more portable.
Say, it&amp;rsquo;s possible to commit it directly into the analysis project&amp;rsquo;s GitHub repo.&lt;/p&gt;
&lt;p&gt;By loading the actual data into DuckDB (without indices), the file grows considerably due to no compression.
Though it is slightly smaller than its SQLite counterpart.
I wonder if this is due to the columnar storage being more space efficient than row storage.
After indexing the DuckDB database, it surprisingly grows to be much larger than SQLite.
I don&amp;rsquo;t know DuckDB&amp;rsquo;s indexing methods enough to understand what happened here.
Since DuckDB is still actively developing its indexing algorithm, I suppose this could be optimized in the future.&lt;/p&gt;
&lt;p&gt;Now we have the databases ready.
Let&amp;rsquo;s see how they perform.&lt;/p&gt;
&lt;!-- cSpell:words dbdir mircrobenchmark noidx EGFR --&gt;

&lt;h2 id="use-duckdb-in-ensembldb"&gt;Use DuckDB in ensembldb&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s painless to tell ensembldb to use DuckDB instead.
&lt;a href="https://duckdb.org/docs/api/r"&gt;DuckDB&amp;rsquo;s R client&lt;/a&gt; already implements R&amp;rsquo;s DBI interface, and ensembldb &lt;a href="https://jorainer.github.io/ensembldb/reference/EnsDb.html"&gt;accepts a DBI connection&lt;/a&gt; to create a EnsDb object.
So we already have everything we need:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;duckdb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ensembldb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;edb_sqlite&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;EnsDb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;EnsDb.Hsapiens.v107.sqlite&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;dbConnect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;duckdb&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;dbdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ensdb_v107.duckdb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;read_only&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;edb_duckdb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;EnsDb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;dbDisconnect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shutdown&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# disconnect after usage&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All the downstream usage of ensembldb is the same from here.&lt;/p&gt;
&lt;h2 id="benchmark-the-databases"&gt;Benchmark the databases&lt;/h2&gt;
&lt;p&gt;Now we have the original SQLite database and three DuckDB databases constructed with various settings ready to use in ensembldb.
Here I tested two scenarios: a genome-wide annotation query and a gene-specific lookup.&lt;/p&gt;
&lt;p&gt;To make the query more realistic and complicated, I also applied a filter to all queries to select annotations only from the canonical chromosomes and remove all LRG genes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;standard_filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;AnnotationFilter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;seq_name&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;X&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;MT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;
        &lt;span class="n"&gt;gene_biotype&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;LRG_gene&amp;#39;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I use &lt;a href="https://cran.r-project.org/web/packages/microbenchmark/index.html"&gt;microbenchmark&lt;/a&gt; to benchmark the same query from different databases.
It works like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mbm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;microbenchmark&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;sqlite_noidx&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;sqlite&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="kc"&gt;...&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;duckdb_parquet&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="kc"&gt;...&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;duckdb&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="kc"&gt;...&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;duckdb_idx&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="kc"&gt;...&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="n"&gt;times&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt;  &lt;span class="c1"&gt;# 50 times for faster queries&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mbm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="genome-wide-annotation-query"&gt;Genome-wide annotation query&lt;/h3&gt;
&lt;p&gt;The first genome-wide query finds the 5&amp;rsquo;UTR genomic ranges of all the transcripts.
This is one of the most computationally intensive built-in queries I know, involving some genomic range arithmics and querying over multiple tables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;five_utr_per_tx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;fiveUTRsByTranscript&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;standard_filter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;five_utr_per_tx&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;## GRangesList object of length 6:&lt;/span&gt;
&lt;span class="c1"&gt;## $ENST00000000442&lt;/span&gt;
&lt;span class="c1"&gt;## GRanges object with 2 ranges and 4 metadata columns:&lt;/span&gt;
&lt;span class="c1"&gt;##       seqnames            ranges strand |   gene_biotype    seq_name&lt;/span&gt;
&lt;span class="c1"&gt;##          &amp;lt;Rle&amp;gt;         &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt; |    &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;&lt;/span&gt;
&lt;span class="c1"&gt;##   [1]       11 64305524-64305736      + | protein_coding          11&lt;/span&gt;
&lt;span class="c1"&gt;##   [2]       11 64307168-64307179      + | protein_coding          11&lt;/span&gt;
&lt;span class="c1"&gt;##               exon_id exon_rank&lt;/span&gt;
&lt;span class="c1"&gt;##           &amp;lt;character&amp;gt; &amp;lt;integer&amp;gt;&lt;/span&gt;
&lt;span class="c1"&gt;##   [1] ENSE00001884684         1&lt;/span&gt;
&lt;span class="c1"&gt;##   [2] ENSE00001195360         2&lt;/span&gt;
&lt;span class="c1"&gt;##   -------&lt;/span&gt;
&lt;span class="c1"&gt;##   seqinfo: 25 sequences (1 circular) from GRCh38 genome&lt;/span&gt;
&lt;span class="c1"&gt;##&lt;/span&gt;
&lt;span class="c1"&gt;## ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here is the microbenchmark results by running the same query in all databases:&lt;/p&gt;
&lt;figure class="invert-in-dark-mode"&gt;
    &lt;img src="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/pics/benchmark_genomewide_5utr_by_tx.png"&gt;
    &lt;figcaption&gt;Benchmark results of extracting genome-wide 5'UTR locations per transcript.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There is a huge performance increase for all DuckDB databases, since this query pretty much scans over the full table.
Overall, DuckDB runs 3+ times faster than SQLite.&lt;/p&gt;
&lt;p&gt;In many cases, there are always a few runs in each database that take significantly more time.
This trend is quite consistent as I re-run the benchmarks multiple times.
While I haven&amp;rsquo;t investigated these outliers, I think this is due to the first run(s) being un-cached.
Surprisingly, DuckDB with indices run much slower than that without indices (especially the first run).
Though the index might be useless in sequential scans, I guess the slowdown could be due to the bigger file (longer to cache) or the query planner accidentally traversing over indices.&lt;/p&gt;
&lt;h2 id="another-genome-wide-annotation-query"&gt;Another genome-wide annotation query&lt;/h2&gt;
&lt;p&gt;The other genome-wide query finds the transcripts of all the genes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tx_per_gene&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;transcriptsBy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;standard_filter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tx_per_gene&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;## GRangesList object of length 6:&lt;/span&gt;
&lt;span class="c1"&gt;## $ENSG00000000003&lt;/span&gt;
&lt;span class="c1"&gt;## GRanges object with 5 ranges and 12 metadata columns:&lt;/span&gt;
&lt;span class="c1"&gt;##       seqnames              ranges strand |           tx_id&lt;/span&gt;
&lt;span class="c1"&gt;##          &amp;lt;Rle&amp;gt;           &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt; |     &amp;lt;character&amp;gt;&lt;/span&gt;
&lt;span class="c1"&gt;##   [1]        X 100633442-100639991      - | ENST00000494424&lt;/span&gt;
&lt;span class="c1"&gt;##   [2]        X 100627109-100637104      - | ENST00000612152&lt;/span&gt;
&lt;span class="c1"&gt;##   [3]        X 100632063-100637104      - | ENST00000614008&lt;/span&gt;
&lt;span class="c1"&gt;##   [4]        X 100627108-100636806      - | ENST00000373020&lt;/span&gt;
&lt;span class="c1"&gt;##   [5]        X 100632541-100636689      - | ENST00000496771&lt;/span&gt;
&lt;span class="c1"&gt;##                 tx_biotype tx_cds_seq_start tx_cds_seq_end         gene_id&lt;/span&gt;
&lt;span class="c1"&gt;##                &amp;lt;character&amp;gt;        &amp;lt;integer&amp;gt;      &amp;lt;integer&amp;gt;     &amp;lt;character&amp;gt;&lt;/span&gt;
&lt;span class="c1"&gt;##   [1] processed_transcript             &amp;lt;NA&amp;gt;           &amp;lt;NA&amp;gt; ENSG00000000003&lt;/span&gt;
&lt;span class="c1"&gt;##   [2]       protein_coding        100630798      100635569 ENSG00000000003&lt;/span&gt;
&lt;span class="c1"&gt;##   [3]       protein_coding        100632063      100635569 ENSG00000000003&lt;/span&gt;
&lt;span class="c1"&gt;##   [4]       protein_coding        100630798      100636694 ENSG00000000003&lt;/span&gt;
&lt;span class="c1"&gt;##   [5] processed_transcript             &amp;lt;NA&amp;gt;           &amp;lt;NA&amp;gt; ENSG00000000003&lt;/span&gt;
&lt;span class="c1"&gt;## ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Similarly, here are the benchmark results:&lt;/p&gt;
&lt;figure class="invert-in-dark-mode"&gt;
    &lt;img src="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/pics/benchmark_genomewide_tx_by_gene.png"&gt;
    &lt;figcaption&gt;Benchmark of extracting genome-wide gene isoforms.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This query tells more or less the same story with only a notable difference.
In this case, fully loaded DuckDB with and without indices share the same performance.
Interestingly, all the DuckDB runtimes are in a bimodal distribution.
I don&amp;rsquo;t know why.&lt;/p&gt;
&lt;h3 id="gene-specific-lookup"&gt;Gene-specific lookup&lt;/h3&gt;
&lt;p&gt;My another main scenario is to look up the annotations of a specific gene.
Let&amp;rsquo;s simulate this kind of queries by retrieving all the transcripts of a gene &amp;ldquo;EGFR&amp;rdquo;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;egfr_tx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;transcripts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;AnnotationFilter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;gene_name&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;EGFR&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;egfr_tx&lt;/span&gt;
&lt;span class="c1"&gt;## GRanges object with 14 ranges and 12 metadata columns:&lt;/span&gt;
&lt;span class="c1"&gt;##                   seqnames            ranges strand |           tx_id&lt;/span&gt;
&lt;span class="c1"&gt;##                      &amp;lt;Rle&amp;gt;         &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt; |     &amp;lt;character&amp;gt;&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000344576        7 55019017-55171037      + | ENST00000344576&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000275493        7 55019017-55211628      + | ENST00000275493&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000455089        7 55019021-55203076      + | ENST00000455089&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000342916        7 55019032-55168635      + | ENST00000342916&lt;/span&gt;
&lt;span class="c1"&gt;##         LRG_304t1        7 55019032-55207338      + |       LRG_304t1&lt;/span&gt;
&lt;span class="c1"&gt;##               ...      ...               ...    ... .             ...&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000450046        7 55109723-55211536      + | ENST00000450046&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000700145        7 55163753-55205865      + | ENST00000700145&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000485503        7 55192811-55200802      + | ENST00000485503&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000700146        7 55198272-55208067      + | ENST00000700146&lt;/span&gt;
&lt;span class="c1"&gt;##   ENST00000700147        7 55200573-55206016      + | ENST00000700147&lt;/span&gt;
&lt;span class="c1"&gt;## ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure class="invert-in-dark-mode"&gt;
    &lt;img src="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/pics/benchmark_extract_specific_gene.png"&gt;
    &lt;figcaption&gt;Benchmark of extracting the annotations of a specific gene.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;SQLite with indices undoubtedly has the best performance.
Understandably, it&amp;rsquo;s been fine tuned for this very use case (extracting a few rows using indices).
And SQLite without an index takes the most time to complete, so it&amp;rsquo;s necessary to always index the tables.&lt;/p&gt;
&lt;p&gt;The performance of all three DuckDB databases fall in between the two extremes of SQLite dbs.
Unlike SQLite, indexed DuckDB only speeds up the query a little bit (21.0ms vs 22.4ms on average).
Given the worse performance of one of the genome-wide queries above using the indexed DuckDB db,
I think it&amp;rsquo;s optional to create indices for ensembldb&amp;rsquo;s DuckDB dbs.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;Here is the overview of the benchmarking results together with the db&amp;rsquo;s file size.
The table below displays the performance in average speed-up ratio (and the worst case ratio) over the original SQLite db (ratio the higher the better):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Database&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Size (%)&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Genome I&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Genome II&lt;/th&gt;
&lt;th style="text-align: right;"&gt;Gene-specific lookup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;SQLite no indexed&lt;/td&gt;
&lt;td style="text-align: right;"&gt;57.9&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.61 (0.78)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.88 (1.02)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.044 (0.12)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;SQLite (original)&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;strong&gt;100.0&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;strong&gt;1.00 (1.00)&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;strong&gt;1.00 (1.00)&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: right;"&gt;&lt;strong&gt;1.00 (1.00)&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;DuckDB w. ext. Parquets&lt;/td&gt;
&lt;td style="text-align: right;"&gt;9.0&lt;/td&gt;
&lt;td style="text-align: right;"&gt;3.36 (3.69)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;4.14 (4.63)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.15 (0.35)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;DuckDB&lt;/td&gt;
&lt;td style="text-align: right;"&gt;40.2&lt;/td&gt;
&lt;td style="text-align: right;"&gt;4.70 (4.76)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;6.30 (6.73)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.61 (0.81)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;DuckDB indexed&lt;/td&gt;
&lt;td style="text-align: right;"&gt;125.7&lt;/td&gt;
&lt;td style="text-align: right;"&gt;3.66 (1.87)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;6.29 (6.33)&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.65 (1.80)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Overall, DuckDB shows impressive performance increase for genome-wide queries.
It uses up less storage too.
While DuckDB is slower than SQLite when it comes to gene-specific lookups, since we are talking about tens of milliseconds per query, unless we are running thousands of these queries, the performance impact is minimal.
On the other hand, genome-wide queries are saving seconds per query.&lt;/p&gt;
&lt;p&gt;As the benchmark results shown, we could replace the original ensembldb database with a DuckDB database by loading the tables and removing the indices.
If the user is willing to sacrifice some performance in gene-specific lookups, DuckDB with external Parquet files only uses &amp;lt; 10% of the original disk space but it still runs faster for genome-wide queries.&lt;/p&gt;
&lt;p&gt;While the default indices copied from SQLite are not very helpful, I didn&amp;rsquo;t tune the indices to maximally speed up the gene-specific lookups.
We can probably also tune the Parquet compression ratio to find a better balance between the decompression speed and file size.
Note that DuckDB&amp;rsquo;s file format is not stabilized yet, so the database needs to be re-created in newer DuckDB versions.&lt;/p&gt;
&lt;p&gt;All in all, I think DuckDB advertises itself accurately when it comes to analytical query workloads.
It shows good performance when it queries a large portion of its content.
By having a similar interface to SQLite and clients in popular languages (R, Python, and etc),
it&amp;rsquo;s easy to change an existing SQLite usecase to use DuckDB.
My small exercise with ensembldb has convinced me to try out DuckDB in more scenarios too.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:sqlite-to-duckdb"&gt;
&lt;p&gt;There is an official extension &lt;a href="https://github.com/duckdblabs/sqlite_scanner"&gt;sqlite_scanner&lt;/a&gt; currently under development that lets a DuckDB attach directly to a SQLite database.
So in the future, it could be much easier to convert SQLite to DuckDB.&amp;#160;&lt;a class="footnote-backref" href="#fnref:sqlite-to-duckdb" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Bioinfo"></category><category term="en"></category><category term="r"></category><category term="python"></category><category term="ensembldb"></category><category term="sqlite"></category><category term="duckdb"></category></entry><entry><title>Build EnsDb from a local Ensembl MySQL database</title><link href="https://blog.liang2.tw/posts/2019/01/build-ensdb-from-local-mysql/" rel="alternate"></link><published>2019-01-08T00:00:00-06:00</published><updated>2022-02-20T20:07:45-06:00</updated><author><name>Liang-Bo Wang</name></author><id>tag:blog.liang2.tw,2019-01-08:/posts/2019/01/build-ensdb-from-local-mysql/</id><summary type="html">&lt;p&gt;In some occasions, I need to access the older version of Ensembl human transcripts. For example, the mutation calls generated by the &lt;a href="https://gdc.cancer.gov/"&gt;NCI&amp;rsquo;s Genomic Data Common&lt;/a&gt; pipeline are annotated by Ensembl v84. To programmatically query the Ensembl annotations, I use the EnsDb SQLite database created by &lt;a href="https://bioconductor.org/packages/release/bioc/html/ensembldb.html"&gt;ensembldb&lt;/a&gt;, which is â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;In some occasions, I need to access the older version of Ensembl human transcripts. For example, the mutation calls generated by the &lt;a href="https://gdc.cancer.gov/"&gt;NCI&amp;rsquo;s Genomic Data Common&lt;/a&gt; pipeline are annotated by Ensembl v84. To programmatically query the Ensembl annotations, I use the EnsDb SQLite database created by &lt;a href="https://bioconductor.org/packages/release/bioc/html/ensembldb.html"&gt;ensembldb&lt;/a&gt;, which is a R package I enjoy using (see &lt;a href="https://blog.liang2.tw/posts/2017/11/use-ensdb-database-in-python/"&gt;my previous post&lt;/a&gt; for its usage).&lt;/p&gt;
&lt;p&gt;The EnsDbs of the recent versions of Ensembl (v87+) are available on AnnotationHub. However, the older versions are not available, and they don&amp;rsquo;t get updated when ensembldb introduces a new feature. For example, now newer EnsDbs include the transcript and gene ID version (&lt;a href="https://github.com/jotsetung/ensembldb/issues/89"&gt;github issue&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In my case, I need to build a EnsDb of Ensembl v84. &lt;a href="https://bioconductor.org/packages/release/bioc/vignettes/ensembldb/inst/doc/ensembldb.html#10_getting_or_building_ensdb_databasespackages"&gt;The ensembldb&amp;rsquo;s documentation&lt;/a&gt; describes how to build one from the public Ensembl MySQL server. However, this method will take more than a day to complete. I started to look for other methods. After some trial and error, I managed to create my EnsDb fast by connecting to a local Ensembl database that I built. Surprisingly the setup wasn&amp;rsquo;t difficult at all, and it only took about an hour to build the EnsDb.&lt;/p&gt;
&lt;p&gt;Here are my notes of how to create the EnsDB from a local Ensembl MySQL database. I use macOS but the steps can be easily modified to work on other OSes.&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#ensembl-vm"&gt;Ensembl VM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#build-a-local-ensembl-v84-mysql-database"&gt;Build a local Ensembl v84 MySQL database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#build-ensdb-from-the-local-mysql-database"&gt;Build EnsDB from the local MySQL database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#remove-mysql"&gt;Remove MySQL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id="ensembl-vm"&gt;Ensembl VM&lt;/h3&gt;
&lt;p&gt;To create a EnsDB from a Ensembl MySQL database, we need to the Ensembl Perl APIs. And the easiest setup is by a &lt;a href="http://www.ensembl.org/info/data/virtual_machine.html"&gt;Ensembl virtual machine&lt;/a&gt;. We just need to import the VM image using VirtualBox and install the ensembldb R package inside the      VM, then it is ready to build the EnsDb. I recommend the VM to have more memory than the default 1GB since a larger memory helps build the R packages and EnsDb.&lt;/p&gt;
&lt;h3 id="build-a-local-ensembl-v84-mysql-database"&gt;Build a local Ensembl v84 MySQL database&lt;/h3&gt;
&lt;p&gt;Ensembl provides &lt;a href="https://www.ensembl.org/info/docs/webcode/mirror/install/ensembl-data.html"&gt;the MySQL database dump&lt;/a&gt; to allow easy import of their data of any version. Assuming the working directory is &lt;code&gt;~/Documents/Ensembl_MySQL_mirror/&lt;/code&gt;, we first copy the database dump by:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~/Documents/Ensembl_MySQL_mirror

&lt;span class="c1"&gt;# Download the db dump&lt;/span&gt;
rsync -a rsync://ftp.ensembl.org/ensembl/pub/release-84/mysql/homo_sapiens_core_84_38 .

&lt;span class="c1"&gt;# MySQL doesn&amp;#39;t accept compressed db dump files so we decompress them&lt;/span&gt;
gunzip *.txt.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While downloading the data, we also need to install the MySQL server. I install &lt;a href="http://www.ensembl.org/info/data/mysql.html"&gt;the same or similar version of MySQL&lt;/a&gt; Ensembl is currently using, which is 5.6 at the time of writing. On macOS, Homebrew can specify the version of MySQL to be installed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;brew install mysql@5.6
&lt;span class="c1"&gt;# And launch the MySQL server&lt;/span&gt;
/usr/local/opt/mysql@5.6/bin/mysql.server start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;First we create a database whose name matches the Ensembl version (v84):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;DATABASE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;homo_sapiens_core_84_38&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then we load the table schema and Ensembl data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/usr/local/opt/mysql@5.6/bin/mysql -u root &lt;span class="se"&gt;\&lt;/span&gt;
    homo_sapiens_core_84_38 &amp;lt; homo_sapiens_core_84_38.sql

/usr/local/opt/mysql@5.6/bin/mysqlimport &lt;span class="se"&gt;\&lt;/span&gt;
    -u root &lt;span class="se"&gt;\&lt;/span&gt;
    --fields-terminated-by&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;\t&amp;#39;&lt;/span&gt; --fields_escaped_by&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;  &lt;span class="se"&gt;\&lt;/span&gt;
    homo_sapiens_core_84_38 -L *.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, we modify the MySQL config at &lt;code&gt;/usr/local/etc/my.cnf&lt;/code&gt; to accept remote database connection, so our VM can access the database on the host machine. I don&amp;rsquo;t use MySQL for anything else, so I simply let MySQL binds to all the possible IP addresses my machine has:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[mysqld]
bind-address = *
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that this is not a secure configuration. To be secure, there should be a designated MySQL user with limited permission and a stricter connection setting. Restart MySQL to load the new config:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/usr/local/opt/mysql@5.6/bin/mysql.server restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Write down an (local) IP address of our host machine.&lt;/p&gt;
&lt;h3 id="build-ensdb-from-the-local-mysql-database"&gt;Build EnsDB from the local MySQL database&lt;/h3&gt;
&lt;p&gt;Now we can come back to the vm and build the EnsDb v84. Run the following R script to build the EnsDb:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ensembldb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;fetchTablesFromEnsembl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="m"&gt;84&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;species&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;human&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;root&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;host&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;lt;our host IP&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3306&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;DBFile&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;makeEnsemblSQLiteFromTables&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The EnsDb SQLite database will be availabe under the working directory. We can test the new EnsDb by:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;edb&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;EnsDb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DBFile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="remove-mysql"&gt;Remove MySQL&lt;/h3&gt;
&lt;p&gt;If there is no other need of MySQL, we can uninstall it and remove all its databases by:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;brew remove mysql@5.6
rm -rf /usr/local/var/mysql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Bioinfo"></category><category term="en"></category><category term="r"></category><category term="bioconductor"></category><category term="sqlite"></category><category term="ensembldb"></category></entry><entry><title>Using EnsDb's annotation database in Python</title><link href="https://blog.liang2.tw/posts/2017/11/use-ensdb-database-in-python/" rel="alternate"></link><published>2017-11-17T00:00:00-06:00</published><updated>2022-05-13T12:53:21-05:00</updated><author><name>Liang-Bo Wang</name></author><id>tag:blog.liang2.tw,2017-11-17:/posts/2017/11/use-ensdb-database-in-python/</id><summary type="html">&lt;p&gt;How to find and download the EnsDb, the Ensembl genomic annotation in SQLite database made by R package ensembldb, and use it in Python application.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I found that there isn&amp;rsquo;t a systematic way to query and convert genomic annotation IDs in Python. At least there isn&amp;rsquo;t one as good as &lt;a href="https://www.bioconductor.org/help/workflows/annotation/annotation/"&gt;what R/Bioconductor currently has&lt;/a&gt;. If you&amp;rsquo;ve never heard of R/Bioconductor annotation tool stack before, check out &lt;a href="https://www.bioconductor.org/help/workflows/annotation/annotation/"&gt;the official workflow&lt;/a&gt; or &lt;a href="https://blog.liang2.tw/posts/2016/05/biocondutor-ensembl-reference/"&gt;my post in 2016&lt;/a&gt; specific for querying Ensembl annotations.&lt;/p&gt;
&lt;p&gt;Although I enjoy using R for genomic annotation conversion, a few days ago I wanted to do the same thing inside my text processing script in Python. I might be able to re-write the script in R but I feel like R is not really the right tool for this task and on top of it, I don&amp;rsquo;t know how to write an efficent text processing in R&lt;sup id="fnref:r-text-processing"&gt;&lt;a class="footnote-ref" href="#fn:r-text-processing"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Knowing the fact that all annotations in R are stored in single-file SQLite databases, I should be able to connect the database directly Python or any other language and wirte SQL query to retrieve the same information. So my question now becomes to how to extract or find the path to the databases. Turn out that many new Bioconductor annotation packages are hosted via &lt;a href="https://bioconductor.org/packages/release/bioc/html/AnnotationHub.html"&gt;AnnotationHub&lt;/a&gt;, and user can search for the annotation package and retrieve them locally by their ID. For example, all the recent Ensembl releases, e.g., &lt;code&gt;EnsDb.Hsapiens.vXX&lt;/code&gt;, are available on AnnotationHub.&lt;/p&gt;
&lt;p&gt;After digging around a bit, I am able to query the AnnotationHub, download the correct EnsDB SQLite database file, and make SQL queries for the annotation ID conversion without any R package. I will share the details in the rest of the post.&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#annotationhub-web-interface"&gt;AnnotationHub web interface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#manual-query-in-annotationhub"&gt;Manual query in AnnotationHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#manual-query-in-ensdb"&gt;Manual query in EnsDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;But before we start with the details, I want to clarify that it wasn&amp;rsquo;t my intention to persuade people away from the current R ecosystem. The current R ecosystem is great and I will recommend people to stick with it as much as you can. I am pretty sure I will hit a lot of issues if I want to do more complex analysis or queries without the help of what R packages provide.&lt;/p&gt;
&lt;h2 id="annotationhub-web-interface"&gt;AnnotationHub web interface&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;EDIT 2019-01-29&lt;/strong&gt;&lt;br&gt;
Now AnnotationHub has a nice &lt;a href="https://annotationhub.bioconductor.org/"&gt;web interface&lt;/a&gt;. With the new API, we can search and download all the EnsDb annotation objects on AnnotationHub by visiting &lt;a href="https://annotationhub.bioconductor.org/package2/AHEnsDbs"&gt;https://annotationhub.bioconductor.org/package2/AHEnsDbs&lt;/a&gt;:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="https://blog.liang2.tw/posts/2017/11/use-ensdb-database-in-python/pics/annotataionhub_web_interface.png"/&gt;
  &lt;figcaption&gt;The web query interface of AnnotationHub&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The following section is the old way to navigate through AnnotationHub&amp;rsquo;s database.&lt;/p&gt;
&lt;h2 id="manual-query-in-annotationhub"&gt;Manual query in AnnotationHub&lt;/h2&gt;
&lt;p&gt;When one wants to use the R package AnnotationHub, the common usage is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AnnotationHub&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ah&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;AnnotationHub&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;## snapshotDate(): 2017-10-27&lt;/span&gt;

&lt;span class="nf"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ah&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;EnsDb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Homo sapiens&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The function call &lt;code&gt;AnnotationHub()&lt;/code&gt; will download the latest version of the metadata of all available annotation object. The subsequent &lt;code&gt;query(...)&lt;/code&gt; function will talk to the local metadata database.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s do it manually without any R function calls.&lt;/p&gt;
&lt;p&gt;The default &lt;a href="https://bioconductor.org/packages/release/bioc/html/AnnotationHub.html"&gt;AnnotationHub&lt;/a&gt; is at &lt;a href="https://annotationhub.bioconductor.org/"&gt;https://annotationhub.bioconductor.org/&lt;/a&gt;. By visiting the page we can find several relevant endpoints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/metadata/annotationhub.sqlite3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/fetch/:id # id =&amp;gt; rdatapaths.id&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So as long as we get the &lt;code&gt;rdatapaths.id&lt;/code&gt; of the EnsDb using the metadata, we can download it via the &lt;code&gt;/fetch/:id&lt;/code&gt; endpoint.&lt;/p&gt;
&lt;p&gt;After downloading the metadata database &lt;code&gt;https://annotationhub.bioconductor.org/metadata/annotationhub.sqlite3&lt;/code&gt;, we can inspect it in SQLite3 by connecting it directly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sqlite3 annotationhub.sqlite3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Some useful commands to inspect a foreign database (or the ultimate help command &lt;code&gt;.help&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;sqlite&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gp"&gt;sqlite&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;mode&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;column&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gp"&gt;sqlite&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="go"&gt;biocversions       rdatapaths         schema_info        test&lt;/span&gt;
&lt;span class="go"&gt;input_sources      recipes            statuses           timestamp&lt;/span&gt;
&lt;span class="go"&gt;location_prefixes  resources          tags&lt;/span&gt;
&lt;span class="gp"&gt;sqlite&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;schema&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rdatapaths&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="go"&gt;CREATE TABLE `rdatapaths`(`id` integer DEFAULT (NULL) NOT NULL PRIMARY KEY , `rdatapath` varchar(255) DEFAULT (NULL) NULL, `rdataclass` varchar(255) DEFAULT (NULL) NULL, `resource_id` integer DEFAULT (NULL) NULL, `dispatchclass` varchar(255) DEFAULT (NULL) NULL, CONSTRAINT `rdatapaths_ibfk_1` FOREIGN KEY (`resource_id`) REFERENCES `resources`(`id`));&lt;/span&gt;
&lt;span class="go"&gt;CREATE INDEX `rdatapaths_resource_id` ON `rdatapaths` (`resource_id`);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So let&amp;rsquo;s make a SQL query to find all Human&amp;rsquo;s EnsDb:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ah_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rdp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;AS&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rdatapaths_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rdp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rdatapath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;AS&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;JOIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rdatapaths&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;AS&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rdp&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;ON&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rdp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resource_id&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;LIKE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%EnsDb for Homo Sapiens%&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;-- ah_id       rdatapaths_id  rdatapath                               title&lt;/span&gt;
&lt;span class="c1"&gt;-- ----------  -------------  --------------------------------------  -- ---------------------------------&lt;/span&gt;
&lt;span class="c1"&gt;-- AH53211     59949          AHEnsDbs/v87/EnsDb.Hsapiens.v87.sqlite  Ensembl 87 EnsDb for Homo Sapiens&lt;/span&gt;
&lt;span class="c1"&gt;-- AH53715     60453          AHEnsDbs/v88/EnsDb.Hsapiens.v88.sqlite  Ensembl 88 EnsDb for Homo Sapiens&lt;/span&gt;
&lt;span class="c1"&gt;-- AH56681     63419          AHEnsDbs/v89/EnsDb.Hsapiens.v89.sqlite  Ensembl 89 EnsDb for Homo Sapiens&lt;/span&gt;
&lt;span class="c1"&gt;-- AH57757     64495          AHEnsDbs/v90/EnsDb.Hsapiens.v90.sqlite  Ensembl 90 EnsDb for Homo Sapiens&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All the Ensembl releases 87+ are available! I will use the release 90 for example. we can download it by its rdatapaths id:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wget -O EnsDb.Hsapiens.v90.sqlite https://annotationhub.bioconductor.org/fetch/64495
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For older Ensembl release, one may need to &lt;a href="https://bioconductor.org/packages/release/bioc/vignettes/ensembldb/inst/doc/ensembldb.html#102_building_annotation_packages"&gt;build the SQLite database based by the instructions from ensembldb&lt;/a&gt;.  For the last GRCh37 release, Ensembl release 75, one can download the source of the Bioconductor annotation package &lt;a href="https://bioconductor.org/packages/release/data/annotation/html/EnsDb.Hsapiens.v75.html"&gt;&lt;code&gt;EnsDb.Hsapiens.v75&lt;/code&gt;&lt;/a&gt; and extract it. The database will be under &lt;code&gt;inst/extdata&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="manual-query-in-ensdb"&gt;Manual query in EnsDB&lt;/h2&gt;
&lt;p&gt;EnsDb SQLite database are Ensembl annotation databases created by the R package &lt;a href="https://bioconductor.org/packages/release/bioc/html/ensembldb.html"&gt;ensembldb&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here I will show how to find a transcript&amp;rsquo;s gene name, its genomic location, and all its exon locations given its Ensembl transcript ID.&lt;/p&gt;
&lt;p&gt;First connect the database by &lt;code&gt;sqlite3 EnsDb.Hsapiens.v90.sqlite&lt;/code&gt;. Its table design is very straightforward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;sqlite&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tables&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="go"&gt;chromosome      exon            metadata        protein_domain  tx2exon&lt;/span&gt;
&lt;span class="go"&gt;entrezgene      gene            protein         tx              uniprot&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So it didn&amp;rsquo;t take me long to figure out how to join the transcript and gene information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tx_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gene_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gene_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;seq_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;seq_strand&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;JOIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;ON&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gene_id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gene_id&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ENST00000358731&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;-- tx_id            gene_id          gene_name   seq_name    seq_strand&lt;/span&gt;
&lt;span class="c1"&gt;-- ---------------  ---------------  ----------  ----------  ----------&lt;/span&gt;
&lt;span class="c1"&gt;-- ENST00000358731  ENSG00000145734  BDP1        5           1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And for the genomic ranges of its exon:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exon_idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exon_seq_start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exon_seq_end&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx2exon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;JOIN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;ON&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx2exon&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exon_id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exon&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exon_id&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;tx_id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ENST00000380139&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;ORDER&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;BY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;exon_idx&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;-- tx_id            exon_idx    exon_seq_start  exon_seq_end&lt;/span&gt;
&lt;span class="c1"&gt;-- ---------------  ----------  --------------  ------------&lt;/span&gt;
&lt;span class="c1"&gt;-- ENST00000380139  1           32427904        32428133&lt;/span&gt;
&lt;span class="c1"&gt;-- ENST00000380139  2           32407645        32407772&lt;/span&gt;
&lt;span class="c1"&gt;-- ENST00000380139  3           32407250        32407338&lt;/span&gt;
&lt;span class="c1"&gt;-- ENST00000380139  4           32404203        32404271&lt;/span&gt;
&lt;span class="c1"&gt;-- ENST00000380139  5           32400723        32403200&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All the coordinates are 1-based and the ranges are inclusive.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;By downloading the underlying annotation database, one can do the same annotation query out of R language and sometimes it may be helpful. I feel like instead of trying to come up with my own layout of annotation mapping across multiple sources, it is more reliable to use a more official build. On the other hand, it is very hard to get the annotation mapping correct and there are tons of corner cases that require careful and systematic decisions. So I don&amp;rsquo;t really recommend to build my own mapping at the first place anyway. The method here should help the situation of annotation query out of R a bit.&lt;/p&gt;
&lt;p&gt;Potentially one can try copy the full R infrastructure but using the same underlying database and replicate the same experience to other languages, but it might require substantial work to get the infrastructure done and correct.&lt;/p&gt;
&lt;p&gt;EDIT 2017-12-13: Add instructions of using older Ensembl release.&lt;br&gt;
EDIT 2019-01-29: Add the web interface of AnnotationHub.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:r-text-processing"&gt;
&lt;p&gt;Based on my impression, my R expert friends would probably recommend me to write it with R-cpp, which I think would be over-kill for such a small task. But my impression can be wrong. Feel free to share your thoughts!&amp;#160;&lt;a class="footnote-backref" href="#fnref:r-text-processing" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Bioinfo"></category><category term="en"></category><category term="python"></category><category term="r"></category><category term="bioconductor"></category><category term="ensembldb"></category></entry></feed>