<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Liang2's Blog - bio</title><link href="https://blog.liang2.tw/" rel="alternate"></link><link href="https://blog.liang2.tw/feeds/tag_bio.atom.xml" rel="self"></link><id>https://blog.liang2.tw/</id><updated>2017-08-10T00:00:00-05:00</updated><entry><title>Use Snakemake on Google cloud</title><link href="https://blog.liang2.tw/posts/2017/08/snakemake-google-cloud/" rel="alternate"></link><published>2017-08-10T00:00:00-05:00</published><updated>2017-08-10T00:00:00-05:00</updated><author><name>Liang-Bo Wang</name></author><id>tag:blog.liang2.tw,2017-08-10:/posts/2017/08/snakemake-google-cloud/</id><summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Run a RNA-seq pipeline using Snakemake locally and later port it to Google Cloud. Snakemake can parallelize jobs of a pipeline and …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Run a RNA-seq pipeline using Snakemake locally and later port it to Google Cloud. Snakemake can parallelize jobs of a pipeline and even across machines.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://snakemake.readthedocs.io/"&gt;Snakemake&lt;/a&gt; has been my favorite workflow management system for a while. I came across it while writing &lt;a href="https://www.dropbox.com/s/u7aa2mbsto77wwy/thesis_upload.pdf?dl=0"&gt;my master thesis&lt;/a&gt; and from the first look, it already appeared to be extremely flexible and powerful. I got some time to play with it during my lab rotation and now after joining the lab, I am using it in my many research projects.  With more and more projects in lab relying on virtualization like &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;, package management like &lt;a href="https://bioconda.github.io/"&gt;bioconda&lt;/a&gt;, and cloud computing like &lt;a href="https://cloud.google.com/"&gt;Google Cloud&lt;/a&gt;, I would like to continue using Snakemake in those scenarios as well. Hence this post to write down all the details.&lt;/p&gt;
&lt;p&gt;The post will introduce the Snakemake by writing the pipeline locally, then gradually move towards to Docker and more Google Cloud products, e.g., Google Cloud Storage, Google Compute Engine (GCE), and Google Container Engine (GKE). &lt;a href="https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html"&gt;Snakemake tutorial&lt;/a&gt; is a good place to start with to understand how Snakemake works.&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#rna-seq-dataset-and-pipeline-for-demonstration"&gt;RNA-seq dataset and pipeline for demonstration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation-of-snakemake-and-all-related-tools"&gt;Installation of snakemake and all related tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#snakemake-local-pipeline-execution"&gt;Snakemake local pipeline execution&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#genome-reference-index-build-how-to-write-snakemake-rules"&gt;Genome reference index build (How to write snakemake rules)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#run-snakemake"&gt;Run Snakemake&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sample-alignment-how-to-write-a-general-rule"&gt;Sample alignment (How to write a general rule)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#transcript-assement"&gt;Transcript assement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#job-dependencies-and-dag"&gt;Job dependencies and DAG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#snakemake-on-google-cloud"&gt;Snakemake on Google Cloud&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#move-input-files-to-the-cloud-from-google-cloud-storage"&gt;Move input files to the cloud (from Google Cloud Storage)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#store-output-files-on-the-cloud"&gt;Store output files on the cloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#dockerize-the-environment"&gt;Dockerize the environment&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#use-google-cloud-storage-in-docker-image"&gt;Use Google Cloud Storage in Docker image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#google-container-engine-gke"&gt;Google Container Engine (GKE)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#potential-issues-of-using-gke-with-snakemake"&gt;Potential issues of using GKE with Snakemake&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id="rna-seq-dataset-and-pipeline-for-demonstration"&gt;RNA-seq dataset and pipeline for demonstration&lt;/h2&gt;
&lt;p&gt;In this example, I will use &lt;code&gt;~/snakemake_example&lt;/code&gt; to store all the files and output. Make sure you change all the paths to be relative to the actual folder in your machine.&lt;/p&gt;
&lt;p&gt;The demo pipeline will be a RNA-seq pipeline for transcript-level expression analysis, often called the &lt;a href="https://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html"&gt;&lt;em&gt;new Tuxedo&lt;/em&gt;&lt;/a&gt; pipeline involving &lt;a href="https://ccb.jhu.edu/software/hisat2/"&gt;HISAT2&lt;/a&gt; and &lt;a href="https://ccb.jhu.edu/software/stringtie/"&gt;StringTie&lt;/a&gt;. The RNA-seq dataset is from &lt;a href="https://github.com/griffithlab/rnaseq_tutorial/"&gt;Griffith Lab&amp;rsquo;s RNA-seq tutorial&lt;/a&gt; which,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;hellip; consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old.&lt;/p&gt;
&lt;p&gt;(From the wiki page &lt;a href="[griffith-lab-data]"&gt;&amp;ldquo;RNA-seq Data&amp;rdquo;&lt;/a&gt; of the tutorial)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our RNA-seq raw data are the 10% downsampled FASTQ files for these samples. For the human genome reference, only the chromosome 22 from GRCh38 is used. The gene annotation is from &lt;a href="http://dec2016.archive.ensembl.org/Homo_sapiens/Info/Index"&gt;Ensembl Version 87&lt;/a&gt;.  Let&amp;rsquo;s download all the samples and annotations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; ~/snakemake_example
&lt;span class="gp"&gt;$&lt;/span&gt; wget https://storage.googleapis.com/lbwang-playground/snakemake_rnaseq/griffithlab_brain_vs_uhr.tar.gz
&lt;span class="gp"&gt;$&lt;/span&gt; tar xf griffithlab_brain_vs_uhr.tar.gz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now you should have the following file structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~/snakemake_example
├── griffithlab_brain_vs_uhr/
│   ├── GRCh38_Ens87_chr22_ERCC/
│   │   ├── chr22_ERCC92.fa
│   │   └── genes_chr22_ERCC92.gtf
│   └── HBR_UHR_ERCC_ds_10pc/
│       ├── HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz
│       ├── HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz
│       ├── ...
│       ├── UHR_Rep3_ERCC-Mix1_Build37-ErccTranscripts-chr22.read1.fastq.gz
│       └── UHR_Rep3_ERCC-Mix1_Build37-ErccTranscripts-chr22.read2.fastq.gz
└── griffithlab_brain_vs_uhr.tar.gz
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="installation-of-snakemake-and-all-related-tools"&gt;Installation of snakemake and all related tools&lt;/h2&gt;
&lt;p&gt;After installing &lt;a href="https://conda.io/miniconda.html"&gt;conda&lt;/a&gt; and setting up &lt;a href="https://bioconda.github.io/"&gt;bioconda&lt;/a&gt;, the installation is simple. All the dependencies are kept in a conda environment called &lt;code&gt;new_tuxedo&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;$&lt;/span&gt; conda create -n new_tuxedo &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="nv"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.6 snakemake hisat2 stringtie samtools
&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;source&lt;/span&gt; activate new_tuxedo        &lt;span class="c1"&gt;# Use the conda env&lt;/span&gt;
&lt;span class="gp"&gt;(new_tuxedo) $&lt;/span&gt; hisat2 --version     &lt;span class="c1"&gt;# Tools are available in the env&lt;/span&gt;
&lt;span class="go"&gt;/Users/liang/miniconda3/envs/new_tuxedo/bin/hisat2-align-s version 2.1.0&lt;/span&gt;
&lt;span class="go"&gt;...&lt;/span&gt;
&lt;span class="gp"&gt;(new_tuxedo) $&lt;/span&gt; deactivate           &lt;span class="c1"&gt;# Exit the env&lt;/span&gt;
&lt;span class="gp"&gt;$&lt;/span&gt; hisat2 --version                  &lt;span class="c1"&gt;# Tools are isolated in the env&lt;/span&gt;
&lt;span class="go"&gt;bash: hisat2: command not found&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;All the following steps should be run inside this conda environment unless it&amp;rsquo;s specified otherwise.&lt;/p&gt;
&lt;h2 id="snakemake-local-pipeline-execution"&gt;Snakemake local pipeline execution&lt;/h2&gt;
&lt;p&gt;The RNA-seq pipeline largely consists of the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build HISAT2 genome reference index for alignment&lt;/li&gt;
&lt;li&gt;Align sample reads to the genome by HISAT2&lt;/li&gt;
&lt;li&gt;Assemble per-sample transcripts by StringTie&lt;/li&gt;
&lt;li&gt;Merge per-sample transcripts by StringTie&lt;/li&gt;
&lt;li&gt;Quantify transcript abundance by StringTie&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To get the taste of how to write a Snakemake pipeline, I will implement it gradually by breaking it into three major parts: genome reference index build, alignment, and transcript assessment.&lt;/p&gt;
&lt;h3 id="genome-reference-index-build-how-to-write-snakemake-rules"&gt;Genome reference index build (How to write snakemake rules)&lt;/h3&gt;
&lt;p&gt;To build the genome reference, we need to extract the splice sites and exons by two of the HISAT2 scripts, &lt;code&gt;hisat2_extract_splice_sites.py&lt;/code&gt; and &lt;code&gt;hisat2_extract_exons.py&lt;/code&gt;. Then we call &lt;code&gt;hisat2-build&lt;/code&gt; to build the index. Create a new file at &lt;code&gt;~/snakemake_example/Snakefile&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;GENOME_FA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/chr22_ERCC92.fa&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;GENOME_GTF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/genes_chr22_ERCC92.gtf&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;HISAT2_INDEX_PREFIX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;extract_genome_splice_sites&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;GENOME_GTF&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.ss&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_extract_splice_sites.py {input} &amp;gt; {output}&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;extract_genome_exons&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;GENOME_GTF&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.exon&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_extract_exons.py {input} &amp;gt; {output}&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;build_hisat_index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;genome_fa&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GENOME_FA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;splice_sites&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.ss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;exons&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.exon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{HISAT2_INDEX_PREFIX}.{{ix}}.ht2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/build.log&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;hisat2-build -p {threads} {input.genome_fa} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;--ss {input.splice_sites} --exon {input.exons} {HISAT2_INDEX_PREFIX} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;2&amp;gt;{log}&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Overall &lt;code&gt;Snakefile&lt;/code&gt; is Python-based, so one can define variables and functions, import Python libraries, and use all the string operations as one does in the Python source code.  Here I defined some constants to the genome reference files (&lt;code&gt;GENOME_FA&lt;/code&gt; and &lt;code&gt;GENOME_GTF&lt;/code&gt;) and the output index prefix (&lt;code&gt;HISAT2_INDEX_PREFIX&lt;/code&gt;) because they will get quite repetitive and specifying them at the front can make future modifications easier.&lt;/p&gt;
&lt;p&gt;In case one hasn&amp;rsquo;t read the &lt;a href="https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html"&gt;Snakemake Tutorial&lt;/a&gt;, here is an overview of the Snakemake pipeline execution.  A Snakemake rule is similar to a Makefile rule.  In a rule, one can specify the input pattern and the output pattern of a rule, as well as the command to run for this rule.  When snakemake runs, all the output user wants to generate will be translated into a sets of rules to be run.  Based on the desired output, Snakemake will find the rule that can generate them (matching the rule&amp;rsquo;s output pattern) and the required input.  The finding process can be traversed rules after rules, that is, some input of a rule depends on the output of another rule, until all the inputs are available.  Then Snakemake will start to generate the output by running the commands each rule gives.&lt;/p&gt;
&lt;p&gt;Now we can look at the three rules in our current &lt;code&gt;Snakefile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The first rule &lt;code&gt;extract_genome_splice_sites&lt;/code&gt; extracts the genome splice sites. The input file is &lt;code&gt;GENOME_GTF&lt;/code&gt; which is the Ensembl gene annotation. The output is a file at &lt;code&gt;hisat2_index/chr22_ERCC92.ss&lt;/code&gt;. The command to generate the output from the given input is a shell command. The command contains some variables, &lt;code&gt;{input}&lt;/code&gt; and &lt;code&gt;{output}&lt;/code&gt;, where Snakemake will fill in them with the sepcified intput and output. So when the first rule is activated, Snakemake will let Bash shell to run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hisat2_extract_splice_sites.py &lt;span class="se"&gt;\&lt;/span&gt;
    griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/genes_chr22_ERCC92.gtf &lt;span class="se"&gt;\&lt;/span&gt;
    &amp;gt; hisat2_index/chr22_ERCC92.ss
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The second rule &lt;code&gt;extract_genome_exons&lt;/code&gt; is quite similar to the first one, but extracts the genome exons and stores it in &lt;code&gt;hisat2_index/chr22_ERCC92.exon&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The third rule &lt;code&gt;build_hisat_index&lt;/code&gt; builds the actual index. Input can be multiple files, in this case there are three entries, including the chromosome sequence, splice sites and exons. One can later refer only to input of the same entry by their entry name. For example, &lt;code&gt;{input.genome_fa}&lt;/code&gt; means the chromosome sequence FASTA file.&lt;/p&gt;
&lt;p&gt;The output of the third rule is &lt;code&gt;expand(f"{HISAT2_INDEX_PREFIX}.{{ix}}.ht2", ix=range(1, 9))&lt;/code&gt;, where &lt;code&gt;expand(...)&lt;/code&gt; is a Snakemake function which can interpolate a string pattern into an array of strings. In this case the generate index files are &lt;code&gt;&amp;lt;index_prefix&amp;gt;.1.ht2&lt;/code&gt;, &amp;hellip; ,&lt;code&gt;&amp;lt;index_prefix&amp;gt;.8.ht2&lt;/code&gt;. Instead of specifies the output eight times, we use &lt;code&gt;expand&lt;/code&gt; and pass a variable &lt;code&gt;ix&lt;/code&gt; to iterate from 1 to 8. The double curly brackets are to escape the &lt;code&gt;f"..."&lt;/code&gt; f-string interpolation (see &lt;a href="https://docs.python.org/3/whatsnew/3.6.html#whatsnew36-pep498"&gt;the Python documentation&lt;/a&gt;). So the whole process to interpret the output is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{HISAT2_INDEX_PREFIX}.{{ix}}.ht2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.{ix}.ht2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.1.ht2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.2.ht2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92.8.ht2&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For the rest of the entries such as &lt;code&gt;threads&lt;/code&gt;, and &lt;code&gt;log&lt;/code&gt;, one can find more information at &lt;a href="https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html"&gt;the Snakemake documentation about Rules&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="run-snakemake"&gt;Run Snakemake&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s build the genome reference index.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;$&lt;/span&gt; snakemake -j &lt;span class="m"&gt;8&lt;/span&gt; -p build_hisat_index
&lt;span class="go"&gt;Provided cores: 8&lt;/span&gt;
&lt;span class="go"&gt;Rules claiming more threads will be scaled down.&lt;/span&gt;
&lt;span class="go"&gt;Job counts:&lt;/span&gt;
&lt;span class="go"&gt;    count   jobs&lt;/span&gt;
&lt;span class="go"&gt;    1   build_hisat_index&lt;/span&gt;
&lt;span class="go"&gt;    1   extract_genome_exons&lt;/span&gt;
&lt;span class="go"&gt;    1   extract_genome_splice_sites&lt;/span&gt;
&lt;span class="go"&gt;    3&lt;/span&gt;

&lt;span class="go"&gt;rule extract_genome_exons:&lt;/span&gt;
&lt;span class="go"&gt;    input: griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/genes_chr22_ERCC92.gtf&lt;/span&gt;
&lt;span class="go"&gt;    output: hisat2_index/chr22_ERCC92.exon&lt;/span&gt;
&lt;span class="go"&gt;    jobid: 1&lt;/span&gt;

&lt;span class="go"&gt;hisat2_extract_exons.py griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/genes_chr22_ERCC92.gtf &amp;gt; hisat2_index/chr22_ERCC92.exon&lt;/span&gt;
&lt;span class="go"&gt;...&lt;/span&gt;
&lt;span class="go"&gt;3 of 3 steps (100%) done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The command &lt;code&gt;snakemake -j 8 -p build_hisat_index&lt;/code&gt; means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-j 8&lt;/code&gt;: Use 8 cores&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-p&lt;/code&gt;: Print the actual command of each job&lt;/li&gt;
&lt;li&gt;&lt;code&gt;build_hisat_index&lt;/code&gt;: The rule or certain output to be generated&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If one runs it again, one will find that snakemake won&amp;rsquo;t do anything since all the output are present and updated.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;$&lt;/span&gt; snakemake -j &lt;span class="m"&gt;8&lt;/span&gt; -p build_hisat_index
&lt;span class="go"&gt;Nothing to be done.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3 id="sample-alignment-how-to-write-a-general-rule"&gt;Sample alignment (How to write a general rule)&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s write the rule to do the sample alignment. Append the &lt;code&gt;Snakefile&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;glob_wildcards&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;griffithlab_brain_vs_uhr/HBR_UHR_ERCC_ds_10pc/{sample}.read1.fastq.gz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;align_hisat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;hisat2_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{HISAT2_INDEX_PREFIX}.{{ix}}.ht2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="n"&gt;fastq1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;griffithlab_brain_vs_uhr/HBR_UHR_ERCC_ds_10pc/{sample}.read1.fastq.gz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;fastq2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;griffithlab_brain_vs_uhr/HBR_UHR_ERCC_ds_10pc/{sample}.read2.fastq.gz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;align_hisat2/{sample}.bam&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;align_hisat2/{sample}.log&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;hisat2 -p {threads} --dta -x {HISAT2_INDEX_PREFIX} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;-1 {input.fastq1} -2 {input.fastq2} 2&amp;gt;{log} | &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;samtools sort -@ {threads} -o {output}&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;align_all_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;align_hisat2/{sample}.bam&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are two rules here but only &lt;code&gt;align_hisat&lt;/code&gt; does the real work. The rule looks familar but there are something new. There is a unresolved variable &lt;code&gt;{sample}&lt;/code&gt; in input, output and log entries, such as &lt;code&gt;fastq1=".../{sample}.read1.fastq.gz"&lt;/code&gt;. So this rule will apply to all outputs that match the pattern &lt;code&gt;align_hisat2/{sample}.bam&lt;/code&gt;. For example, given an output &lt;code&gt;align_hisat2/mysample.bam&lt;/code&gt;, Snakemake will look for the inputs &lt;code&gt;griffithlab_brain_vs_uhr/HBR_UHR_ERCC_ds_10pc/mysample.read1.fastq.gz&lt;/code&gt;, where &lt;code&gt;sample = "mysample"&lt;/code&gt; in this case.&lt;/p&gt;
&lt;p&gt;To get the names of all the samples, we use &lt;code&gt;glob_wildcards(...)&lt;/code&gt; which finds all the files that match the given string pattern, and collects the possible values of the variables in the string pattern as a list. Hence all the sample names are stored in &lt;code&gt;SAMPLES&lt;/code&gt;, and the other rule takes input of all samples&amp;rsquo; BAM files to generate alignment of all samples.&lt;/p&gt;
&lt;p&gt;Now run Snakemake again with a different rule target:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;snakemake -j 8 -p align_all_samples
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This time pay attention to the CPU usage (say, using &lt;a href="http://hisham.hm/htop/"&gt;&lt;code&gt;htop&lt;/code&gt;&lt;/a&gt;), one should find out that snakemake runs jobs in parallel, and tries to use as many cores as possible.&lt;/p&gt;
&lt;h3 id="transcript-assement"&gt;Transcript assement&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s complete the whole pipeline by adding all StringTie steps to &lt;code&gt;Snakefile&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;stringtie_assemble&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;genome_gtf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GENOME_GTF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;bam&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;align_hisat2/{sample}.bam&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;stringtie/assembled/{sample}.gtf&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;stringtie -p {threads} -G {input.genome_gtf} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;-o {output} -l {wildcards.sample} {input.bam}&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;stringtie_merge_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stringtie/assembled/{sample}.gtf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;stringtie/merged_list.txt&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;gtf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gtf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;stringtie_merge&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;genome_gtf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GENOME_GTF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;merged_list&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stringtie/merged_list.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;sample_gtfs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stringtie/assembled/{sample}.gtf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;stringtie/merged.gtf&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;stringtie --merge -p {threads} -G {input.genome_gtf} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;-o {output} {input.merged_list}&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;stringtie_quant&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;merged_gtf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stringtie/merged.gtf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;sample_bam&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;align_hisat2/{sample}.bam&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;gtf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stringtie/quant/{sample}/{sample}.gtf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;ctabs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;stringtie/quant/{{sample}}/{name}.ctab&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;i2t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;e2t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;i_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;e_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;t_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;stringtie -e -B -p {threads} -G {input.merged_gtf} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;-o {output.gtf} {input.sample_bam}&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;quant_all_samples&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stringtie/quant/{sample}/{sample}.gtf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Most rules are similar to the previous ones except for &lt;code&gt;stringtie_merge_list&lt;/code&gt;. This step a file is generated to contain list of paths to all the samples&amp;rsquo; GTF file. Instead of running some command (no &lt;code&gt;shell&lt;/code&gt; entry), a &lt;code&gt;run&lt;/code&gt; entry is used to write a Python code snippet to generate the file.&lt;/p&gt;
&lt;p&gt;Another thing to be noted is the output entry &lt;code&gt;ctabs=...&lt;/code&gt; of &lt;code&gt;stringtie_quant&lt;/code&gt;. The following lines are equivalent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Before expansion&lt;/span&gt;
&lt;span class="n"&gt;ctabs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;stringtie/quant/{{sample}}/{name}.ctab&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;i2t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;e2t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;i_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;e_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;t_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# After expansion&lt;/span&gt;
&lt;span class="n"&gt;ctabs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stringtie/quant/{sample}/i2t.ctab&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;stringtie/quant/{sample}/e2t.ctab&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;stringtie/quant/{sample}/t_data.ctab&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The full &lt;code&gt;Snakefile&lt;/code&gt; can be found &lt;a href="https://gist.github.com/ccwang002/2659b19439b6205284c0ae68ca06345d"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="job-dependencies-and-dag"&gt;Job dependencies and DAG&lt;/h3&gt;
&lt;p&gt;Now with the pipeline complete, we can further look at the how all the rules are chained with each other. Snakemake has a command to generate the job depedency graph (a DAG):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;snakemake --dag quant_all_samples | dot -Tsvg &amp;amp;gt; dag.svg
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure full-img"&gt;
  &lt;img src="https://blog.liang2.tw/posts/2017/08/snakemake-google-cloud/pics/snakemake_rnaseq_dag.svg"/&gt;
  &lt;p class="caption"&gt;Snakemake job dependency graph.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Snakemake generates such DAG first before execution, where each node represents a job. As long as two nodes have no connected edges and their input exist, they can be executed parallely. This is a powerful feature to pipeline management, which can use the resources in a fin grain.&lt;/p&gt;
&lt;p&gt;A simpler graph that shows rules instead of jobs can be generated by:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;snakemake --rulegraph quant_all_samples | dot -Tsvg &amp;amp;gt; ruledag.svg
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
  &lt;img src="https://blog.liang2.tw/posts/2017/08/snakemake-google-cloud/pics/snakemake_rnaseq_ruledag.svg"/&gt;
  &lt;p class="caption"&gt;Snakemake rule dependency graph.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id="snakemake-on-google-cloud"&gt;Snakemake on Google Cloud&lt;/h2&gt;
&lt;p&gt;Now we start to move our Snakemake pipeline to the Google Cloud. To complete all the following steps, one needs a Google account and has a bucket on the Google Cloud with write access. That is, be able to upload the output back to Google Cloud Storage. Snakemake is able to download/upload files from the cloud, one needs to &lt;a href="https://cloud.google.com/sdk/downloads"&gt;set up the Google Cloud SDK on the local machine&lt;/a&gt; and create the default application credentials:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud auth application-default login
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Also, install the neccessary Python packages to give Snakemake the access to storage API:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install google-cloud-storage
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Actually snakemake support remote files from many more providers. More detail can be found at &lt;a href="https://snakemake.readthedocs.io/en/stable/snakefiles/remote_files.html"&gt;the Snakemake documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that although one can run this section on a local machine, this step will be significantly faster if one runs it on a Google Computer Engine (GCE) instance. It also saves extra bandwidth and fees.&lt;/p&gt;
&lt;h3 id="move-input-files-to-the-cloud-from-google-cloud-storage"&gt;Move input files to the cloud (from Google Cloud Storage)&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s modify the &lt;code&gt;Snakefile&lt;/code&gt; to use the reference and FASTQ files from Google Cloud Storage. Replace those file paths with the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;snakemake.remote.GS&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RemoteProvider&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;GSRemoteProvider&lt;/span&gt;
&lt;span class="n"&gt;GS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GSRemoteProvider&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;GS_PREFIX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;lbwang-playground/snakemake_rnaseq&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;GENOME_FA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;GS&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{GS_PREFIX}/griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/chr22_ERCC92.fa&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;GENOME_GTF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GS&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{GS_PREFIX}/griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/genes_chr22_ERCC92.gtf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;HISAT2_INDEX_PREFIX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;SAMPLES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GS&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob_wildcards&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GS_PREFIX&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/griffithlab_brain_vs_uhr/HBR_UHR_ERCC_ds_10pc/{sample}.read1.fastq.gz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# rule extract_genome_splice_sites:&lt;/span&gt;
&lt;span class="c1"&gt;# ...&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;align_hisat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;hisat2_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;expand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{HISAT2_INDEX_PREFIX}.{{ix}}.ht2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="n"&gt;fastq1&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GS&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GS_PREFIX&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/griffithlab_brain_vs_uhr/HBR_UHR_ERCC_ds_10pc/{sample}.read1.fastq.gz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;fastq2&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GS&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remote&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GS_PREFIX&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/griffithlab_brain_vs_uhr/HBR_UHR_ERCC_ds_10pc/{sample}.read2.fastq.gz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now all the file paths are on Google Cloud Storage under the bucket &lt;code&gt;lbwang-playground&lt;/code&gt;. For example, &lt;code&gt;GENOME_FA&lt;/code&gt; points to &lt;code&gt;gs://lbwang-playground/snakemake_rnaseq/griffithlab_brain_vs_uhr/GRCh38_Ens87_chr22_ERCC/chr22_ERCC92.fa&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One could launch Snakemake again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;snakemake --timestamp -p --verbose --keep-remote -j 8 quant_all_samples
&lt;/pre&gt;&lt;/div&gt;


&lt;h3 id="store-output-files-on-the-cloud"&gt;Store output files on the cloud&lt;/h3&gt;
&lt;p&gt;Although we could replace all the file paths to &lt;code&gt;GS.remote(...)&lt;/code&gt;, there is a simpler way to replace every path through the command line option. On top of that, we need to add a &lt;code&gt;FULL_HISAT2_INDEX_PREFIX&lt;/code&gt; variable to reflect the path change that prepends the path under the writable bucket. Replace all &lt;code&gt;{WRITABLE_BUCKET_PATH}&lt;/code&gt; with a writable Google Cloud Storage bucket.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;HISAT2_INDEX_PREFIX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hisat2_index/chr22_ERCC92&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;FULL_HISAT2_INDEX_PREFIX&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;{WRITABLE_BUCKET_PATH}/hisat2_index/chr22_ERCC92&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;build_hisat_index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;hisat2-build -p {threads} {input.genome_fa} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;--ss {input.splice_sites} --exon {input.exons} {FULL_HISAT2_INDEX_PREFIX} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;2&amp;gt;{log}&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;align_hisat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;hisat2 -p {threads} --dta -x {FULL_HISAT2_INDEX_PREFIX} &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;-1 {input.fastq1} -2 {input.fastq2} 2&amp;gt;{log} | &amp;quot;&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;samtools sort -@ {threads} -o {output}&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The full &lt;code&gt;Snakefile&lt;/code&gt; can be found &lt;a href="https://gist.github.com/ccwang002/2686840e90574a67a673ec4b48e9f036"&gt;here&lt;/a&gt;. Now run the Snakemake with the following options:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;snakemake --timestamp -p --verbose --keep-remote -j &lt;span class="m"&gt;8&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
        --default-remote-provider GS &lt;span class="se"&gt;\&lt;/span&gt;
        --default-remote-prefix &lt;span class="o"&gt;{&lt;/span&gt;WRITABLE_BUCKET_PATH&lt;span class="o"&gt;}&lt;/span&gt; &amp;gt; &lt;span class="se"&gt;\&lt;/span&gt;
        quant_all_samples
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To understand how the whole remote files work, here is the the folder structure after the exection:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~/snakemake_example
├── lbwang-playground/
│   └── snakemake_rnaseq/
│       └── griffithlab_brain_vs_uhr/
│           ├── GRCh38_Ens87_chr22_ERCC/
│           └── HBR_UHR_ERCC_ds_10pc/
├── {WRITABLE_BUCKET_PATH}/
│   ├── align_hisat2/
│   ├── hisat2_index/
│   └── stringtie/
└── Snakefile
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So Snakemake simply downloads/generates the files with the full path on remote storage.&lt;/p&gt;
&lt;h2 id="dockerize-the-environment"&gt;Dockerize the environment&lt;/h2&gt;
&lt;p&gt;Although bioconda has made the package installation very easy, it would be easier to just isolate the whole environment at the operating system level. One common approach is to use &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A minimal working Dockerfile would be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="s"&gt; continuumio/miniconda3&lt;/span&gt;
&lt;span class="k"&gt;RUN&lt;/span&gt; conda install -y &lt;span class="nv"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.6 nomkl &lt;span class="se"&gt;\&lt;/span&gt;
        stringtie samtools hisat2 snakemake google-cloud-storage &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; conda clean -y --all
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;However there are some details required extra care at the time of writing, so I&amp;rsquo;ve created a Docker image for this pipeline on Docker Hub, &lt;a href="https://hub.docker.com/r/lbwang/snakemake-conda-rnaseq/"&gt;&lt;code&gt;lbwang/snakemake-conda-rnaseq&lt;/code&gt;&lt;/a&gt;. One could be able to run the snakemake by:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~/snakemake_example
docker run -t                       &lt;span class="se"&gt;\&lt;/span&gt;
    -v &lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;:/analysis             &lt;span class="se"&gt;\&lt;/span&gt;
    lbwang/snakemake-conda-rnaseq   &lt;span class="se"&gt;\&lt;/span&gt;
    snakemake -j &lt;span class="m"&gt;2&lt;/span&gt; --timestamp      &lt;span class="se"&gt;\&lt;/span&gt;
        -s /analysis/Snakefile --directory /analysis &lt;span class="se"&gt;\&lt;/span&gt;
        quant_all_samples
&lt;/pre&gt;&lt;/div&gt;


&lt;h3 id="use-google-cloud-storage-in-docker-image"&gt;Use Google Cloud Storage in Docker image&lt;/h3&gt;
&lt;p&gt;To use Google&amp;rsquo;s Cloud products in a Docker image, one needs to install &lt;a href="https://cloud.google.com/sdk/downloads"&gt;Google Cloud SDK&lt;/a&gt; inside the Docker image. Refer to &lt;a href="https://github.com/GoogleCloudPlatform/cloud-sdk-docker/blob/master/debian_slim/Dockerfile"&gt;Google&amp;rsquo;s Dockerfile with Cloud SDK&lt;/a&gt; for detail. &lt;a href="https://hub.docker.com/r/lbwang/snakemake-conda-rnaseq/"&gt;&lt;code&gt;lbwang/snakemake-conda-rnaseq&lt;/code&gt;&lt;/a&gt; has installed the Cloud SDK.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo docker run -t -i                           &lt;span class="se"&gt;\&lt;/span&gt;
    -v &lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;:/analysis                         &lt;span class="se"&gt;\&lt;/span&gt;
    -v ~/.config/gcloud:/root/.config/gcloud    &lt;span class="se"&gt;\&lt;/span&gt;
    lbwang/snakemake-conda-rnaseq               &lt;span class="se"&gt;\&lt;/span&gt;
    snakemake -j &lt;span class="m"&gt;4&lt;/span&gt; --timestamp --verbose -p --keep-remote   &lt;span class="se"&gt;\&lt;/span&gt;
        -s /analysis/Snakefile --directory /analysis        &lt;span class="se"&gt;\&lt;/span&gt;
        --default-remote-provider GS --default-remote-prefix &lt;span class="s2"&gt;&amp;quot;{WRITABLE_BUCKET_PATH}&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
        quant_all_samples
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To run Docker on a GCE VM instance, it requires the host machine (the VM instance) to have Docker installed. One may refer to Docker&amp;rsquo;s &lt;a href="https://docs.docker.com/engine/installation/linux/docker-ce/debian/#install-using-the-repository"&gt;official installation guide&lt;/a&gt; to install it. VM instance by default inherit the user&amp;rsquo;s permission (via the automatically created service account), thus the command above should apply to the GCE instance as well.&lt;/p&gt;
&lt;h2 id="google-container-engine-gke"&gt;Google Container Engine (GKE)&lt;/h2&gt;
&lt;p&gt;To scale up the pipeline execution across multiple machines, Snakemake could use &lt;a href="https://cloud.google.com/container-engine/"&gt;Google Container Engine&lt;/a&gt; (GKE, implemented on top of Kubernetes). This method is built on Docker which each node will pull down the given Docker image to load the environment. After &lt;a href="https://bitbucket.org/snakemake/snakemake/issues/602"&gt;some discussions&lt;/a&gt; about how to specify user input image &lt;sup id="fnref:kubernetes-docker"&gt;&lt;a class="footnote-ref" href="#fn:kubernetes-docker" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;, on Snakemake 4.1+ one is able to specify the Docker image Kubernete&amp;rsquo;s node uses by &lt;code&gt;--container-image &amp;lt;image&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To install the master branch of Snakemake, run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install git+https://bitbucket.org/snakemake/snakemake.git@master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Following Snakemake&amp;rsquo;s &lt;a href="https://snakemake.readthedocs.io/en/stable/executable.html#executing-a-snakemake-workflow-via-kubernetes"&gt;GKE guide&lt;/a&gt;, extra packages need to be installed to talk to GKE (Kubernetes) cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install kubernetes
gcloud components install kubectl
&lt;span class="c1"&gt;# or Debian on GCE:&lt;/span&gt;
&lt;span class="c1"&gt;# sudo apt-get install kubectl&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First we create the GKE cluster by:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;CLUSTER_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;snakemake-cluster&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;ZONE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;us-central1-a&amp;quot;&lt;/span&gt;
gcloud container clusters create &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --zone&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$ZONE&lt;/span&gt; --num-nodes&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --machine-type&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;n1-standard-4&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --scopes storage-rw
gcloud container clusters get-credentials --zone&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$ZONE&lt;/span&gt; &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will launch 3 GCE VM instances using &lt;code&gt;n1-standard-4&lt;/code&gt; machine type (4 CPUs). Therefore in the cluster there are total 12 CPUs available for computation. Modify the variables to fit one&amp;rsquo;s setting.&lt;/p&gt;
&lt;p&gt;Note that some rule may specify a number of CPUs that no node in the clusters has, say the rule &lt;code&gt;build_hisat_index&lt;/code&gt; specifies 8 threads. In this case, the cluster cannot find a node with enough free CPUs to forward the job to a &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/"&gt;pod&lt;/a&gt; and the cluster will halt. Therefore, make sure to lower the &lt;code&gt;threads&lt;/code&gt; to a reasonable number (or use &lt;a href="https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html"&gt;configfile&lt;/a&gt; to apply to mulitple samples). We will continue to use the same Docker image &lt;a href="https://hub.docker.com/r/lbwang/snakemake-conda-rnaseq/"&gt;&lt;code&gt;lbwang/snakemake-conda-rnaseq&lt;/code&gt;&lt;/a&gt; as the Kubernetes&amp;rsquo; container image.&lt;/p&gt;
&lt;p&gt;By default, Snakemake will always check if the output files are outdated, that is, older than the rule that generated them. To ensure it re-runs the pipeline, one might need to remove the generated output before calling Snakemake again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gsutil -m rm -r gs://&lt;span class="o"&gt;{&lt;/span&gt;WRITABLE_BUCKET_PATH&lt;span class="o"&gt;}&lt;/span&gt;/&lt;span class="o"&gt;{&lt;/span&gt;align_hisat2,hisat2_index,stringtie&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then we are able to run the pipeline again.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;snakemake                                            &lt;span class="se"&gt;\&lt;/span&gt;
    --timestamp -p --verbose --keep-remote           &lt;span class="se"&gt;\&lt;/span&gt;
    -j &lt;span class="m"&gt;12&lt;/span&gt; --kubernetes                               &lt;span class="se"&gt;\&lt;/span&gt;
    --container-image lbwang/snakemake-conda-rnaseq &lt;span class="se"&gt;\&lt;/span&gt;
    --default-remote-provider GS                     &lt;span class="se"&gt;\&lt;/span&gt;
    --default-remote-prefix &lt;span class="o"&gt;{&lt;/span&gt;WRITABLE_BUCKET_PATH&lt;span class="o"&gt;}&lt;/span&gt;   &lt;span class="se"&gt;\&lt;/span&gt;
    quant_all_samples
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that since we change the container image, we have to make sure the version of Snakemake in the Docker image and the machine starting the pipeline matches. An easy way to ensure that the versions are matched is to start the workflow inside the same Docker image.&lt;/p&gt;
&lt;p&gt;To connect the Kubernete cluster inside Docker, we need to pass kubectl&amp;rsquo;s config file as well, which is at &lt;code&gt;~/.kube/config&lt;/code&gt;. So the full command becomes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo docker run -t -i                           &lt;span class="se"&gt;\&lt;/span&gt;
    -v &lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;:/analysis                         &lt;span class="se"&gt;\&lt;/span&gt;
    -v ~/.config/gcloud:/root/.config/gcloud    &lt;span class="se"&gt;\&lt;/span&gt;
    -v ~/.kube/config:/root/.kube/config        &lt;span class="se"&gt;\&lt;/span&gt;
    lbwang/snakemake-conda-rnaseq               &lt;span class="se"&gt;\&lt;/span&gt;
    snakemake                                           &lt;span class="se"&gt;\&lt;/span&gt;
        -s /analysis/Snakefile --directory /analysis    &lt;span class="se"&gt;\&lt;/span&gt;
        --timestamp -p --verbose --keep-remote          &lt;span class="se"&gt;\&lt;/span&gt;
        -j &lt;span class="m"&gt;12&lt;/span&gt; --kubernetes                              &lt;span class="se"&gt;\&lt;/span&gt;
        --container-image lbwang/snakemake-conda-rnaseq &lt;span class="se"&gt;\&lt;/span&gt;
        --default-remote-provider GS                    &lt;span class="se"&gt;\&lt;/span&gt;
        --default-remote-prefix &lt;span class="o"&gt;{&lt;/span&gt;WRITABLE_BUCKET_PATH&lt;span class="o"&gt;}&lt;/span&gt;  &lt;span class="se"&gt;\&lt;/span&gt;
        quant_all_samples
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After running our pipeline, make sure to delete the GKE cluster by:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gcloud container clusters delete --zone&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$ZONE&lt;/span&gt; &lt;span class="nv"&gt;$CLUSTER_NAME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3 id="potential-issues-of-using-gke-with-snakemake"&gt;Potential issues of using GKE with Snakemake&lt;/h3&gt;
&lt;p&gt;I still encountered the following issues while running the whole pipeline on the Kubernetes. It is likely that they are not Snakemake&amp;rsquo;s fault but I couldn&amp;rsquo;t find enough time to dig into the details at the time of writing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HISAT2 cannot build its index on Kubenetes. So the step &lt;code&gt;build_hisat_index&lt;/code&gt; failed for unknown reason. The error message from HISAT2 looks like this:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;...
Wrote 8912688 bytes to secondary GFM file: {WRITABLE_BUCKET_PATH}/snakemake_demo/hisat2_index/chr22_ERCC92.6.ht2
Index is corrupt: File size for {WRITABLE_BUCKET_PATH}/snakemake_demo/hisat2_index/chr22_ERCC92.6.ht2 should have been 8912688 but is actually 0.
Please check if there is a problem with the disk or if disk is full.
Total time for call to driver() for forward index: 00:01:18
Error: Encountered internal HISAT2 exception (#1)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;Snakemake is a flexible pipeline management tool that can be run locally and on the cloud. Although it is able to run on Kubernetes such as Google Container Engine, it is a relatively new feature and will take some time to stablize. Currently if one wants to run everything (both the computing and the data) on the cloud, using Google Compute Engine and Google Cloud Storage will be the way to go.&lt;/p&gt;
&lt;p&gt;Using a 4-core (n1-standard-4) GCE instance, the total time to finish the pipeline locally and via Google Cloud Storage were 3.2 mins and 5.8 mins resepctively. So there are some overhead to transfer files from/to the storage.&lt;/p&gt;
&lt;p&gt;Docker and bioconda have made the deployment a lot easier. Bioconda truly saves a lot of duplicated efforts to figure out the tool compilation. Docker provides an OS-level isolation and an ecosystem of deployment. With more tools such as &lt;a href="http://singularity.lbl.gov/"&gt;Singularity&lt;/a&gt; continuing to come out, virtualization seems to be a inevitable trend.&lt;/p&gt;
&lt;p&gt;Other than Google cloud products, Snakemake also supports AWS, S3, LSF, SLURM and many other cluster settings. It seems to me that the day when one &lt;code&gt;Snakefile&lt;/code&gt; works for all platforms might be around the corner.&lt;/p&gt;
&lt;p&gt;EDIT 2017-08-15: Add a section about using Google Cloud in Docker. Update summary with some time measurements. Add links to the full Snakefiles.&lt;br&gt;
EDIT 2017-09-07: Snakemake has added the support of custom Kubernetes container image. Thus update the GKE section to use the official parameter to pass image.&lt;br&gt;
EDIT 2017-11-17: Add instructions to run the Snakemake on Kubernete inside Docker. And also list out the issues of using GKE.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:kubernetes-docker"&gt;
&lt;p&gt;In the discussion, Snakemake&amp;rsquo;s author, Johannes, mentioned the possiblity of using &lt;a href="http://singularity.lbl.gov/"&gt;Singularity&lt;/a&gt; so each rule can run in a different virutal environment. Singularity support comes at Snakemake 4.2+.&amp;#160;&lt;a class="footnote-backref" href="#fnref:kubernetes-docker" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="en"></category><category term="bio"></category><category term="python"></category><category term="snakemake"></category><category term="cloud"></category></entry><entry><title>Variants、eQTL、MPRA</title><link href="https://blog.liang2.tw/posts/2017/06/variants-eqtl-mpra/" rel="alternate"></link><published>2017-06-20T00:00:00-05:00</published><updated>2017-06-20T00:00:00-05:00</updated><author><name>Liang-Bo Wang</name></author><id>tag:blog.liang2.tw,2017-06-20:/posts/2017/06/variants-eqtl-mpra/</id><summary type="html">&lt;p&gt;本文內容主要來自 Barak Cohen 教授給的數堂課的筆記，以 Systems Biology 的角度來看 coding/noncoding variant modeling 和相關實驗 MPRA。&lt;/p&gt;</summary><content type="html">&lt;p&gt;Computational Biology 和 Bioinformatics 在現在可能區分不大，本文也不打算深究兩定義，但他們大致能代表兩大類將電腦科學、程式運用在生物上的研究。&lt;/p&gt;
&lt;p&gt;在碩班，我的實驗室一直鼓勵我們去想新的演算法，把某種預測做得更好或者快，或者運用更多來源的數據；做新的工具；整合出新的資料庫。這些應用都有他們的研究價值，也需要大量的技術投入，即便在發表上並不會放入這些細節。這類研究比較偏向 Bioinformatics。&lt;/p&gt;
&lt;p&gt;來 WashU 前，我期許自己繼續往 Bioinformatics 深入。然而，在過去的數月裡，即便我仍投入在這些數據分析與工具開發上，另一大部份的時間，我經歷了許多關於模型，或者，關於「如何回答重要的生物問題」上的討論，有了較碩班訓練不同的啟發。這另一類研究比較偏向 Computational Biology。&lt;/p&gt;
&lt;p&gt;本文想用另一個角度來看所謂的「modeling」。內容主要來自 &lt;a href="http://genetics.wustl.edu/bclab/"&gt;Barak Cohen&lt;/a&gt; 教授給的數堂課的筆記，主題為 &lt;em&gt;Coding and Noncoding Variant&lt;/em&gt;。我生物背景不足，如果筆記有任何錯誤，煩請告知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conflict of Interest&lt;/strong&gt;: Cohen Lab 開發了 &lt;a href="http://www.pnas.org/content/109/47/19498.short"&gt;CRE-seq&lt;/a&gt; (&lt;em&gt;cis&lt;/em&gt;-regulatory element by sequencing)，其中一種 MPRA (Massively Parallel Reporter Assay) 技術。&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#coding-vs-noncoding-variants"&gt;Coding vs noncoding variants&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#noncoding-elements"&gt;Noncoding elements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#noncoding-variants"&gt;Noncoding variants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#endophenotypes"&gt;Endophenotypes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#eqtl"&gt;eQTL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#mpra"&gt;MPRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id="coding-vs-noncoding-variants"&gt;Coding vs noncoding variants&lt;/h2&gt;
&lt;p&gt;首先來談談 coding 和 noncoding variant。課堂上老師讓我們自由辯論研究兩者的「優缺點」，亦或，如果你是 PI 比較想研究哪個，面臨的優勢與困境。&lt;/p&gt;
&lt;p&gt;Coding variant 很好理解，就是在某個 gene coding region 產生的序列改變，一般會先看所謂的 nonsynonymous，即這個 variant 造成 amino acid 改變，影響到蛋白質的結構，進而影響到其功能。synonymous variant 雖然不會改變 amino acid，但在模式物種中，可能會討論不同 amino acid 對於不同 tRNA 的偏好，也許會影響到 gene expression。另一方面，它也可能會影響 transcription factor (TF) binding，某些 TF 在 biding 有偏好的 DNA sequence（motif），即使蛋白質序列不變，TF binding 變化也會影響到其他基因的調控。&lt;/p&gt;
&lt;p&gt;不過一般而言，coding variant 主要都是考慮 nonsynonymous change，這造成的變化十分具大，無法解釋像 complex traits、gene expression 高低這種細微的變化。&lt;/p&gt;
&lt;h3 id="noncoding-elements"&gt;Noncoding elements&lt;/h3&gt;
&lt;p&gt;Noncoding vairant 相對而言複雜的多。在討論它之前，不如來說說看我們知道哪些 noncoding elements：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introns&lt;/li&gt;
&lt;li&gt;Promoters&lt;/li&gt;
&lt;li&gt;Regulatory elements (REs)&lt;ul&gt;
&lt;li&gt;&lt;em&gt;cis&lt;/em&gt;-regulatory elements (CREs): promoters, enhancers&lt;/li&gt;
&lt;li&gt;&lt;em&gt;trans&lt;/em&gt;-regulatory elements&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;miRNAs&lt;/li&gt;
&lt;li&gt;Retrovirus, satellites, centromeres, telemeres&lt;/li&gt;
&lt;li&gt;Structual elements &lt;ul&gt;
&lt;li&gt;Matrix Attachment Regions (MARs)&lt;/li&gt;
&lt;li&gt;Lamina Associated Domains (LADs)&lt;/li&gt;
&lt;li&gt;CTCF/Cohesin&lt;/li&gt;
&lt;li&gt;Topologically associating domains (TADs)&lt;/li&gt;
&lt;li&gt;3D genome&lt;sup id="fnref:3D genome"&gt;&lt;a class="footnote-ref" href="#fn:3D genome" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Methylation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;咦，忘記提到 histone modification 嗎？關於這些 epigenetics markers，Barak 對於他們有深刻的懷疑，他認為這些只是 markers 而非最終 regulatory element：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Something you can measure does not mean it is interesting.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然後建議我們去讀一篇批評 ENCODE 的論文&lt;sup id="fnref:ENCODE paper"&gt;&lt;a class="footnote-ref" href="#fn:ENCODE paper" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;，被他評之為近十年最辛辣，標題也非常有趣。&lt;/p&gt;
&lt;h3 id="noncoding-variants"&gt;Noncoding variants&lt;/h3&gt;
&lt;p&gt;從 non-coding elements 我們能知道控制 gene expression 可以從很多面向切入，於是討論 non-coding variant 時就會有很多不同的機制影響 gene expression。底下針對所謂的 enhancers (RE) 和 promoters 來畫個簡單的示意圖：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;     TF1  TF2  TF3         TF4  [RNA PolII]
----[  enhancer  ]-------[promoter]--[gene body]-------------------------
  &amp;lt;-- Topologically Associating Domain, TAD -----&amp;gt;   &amp;lt;-- Another TAD --&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;RNA Polymerase II (RNA PolII) 負責 gene transcription，而 promoter 是一段在 gene body 前不特定的序列可以吸引 RNA PolII 來提高 gene transcription rate，很有可能就會提高 gene expression。TF 可能會辨別 promoter 上特定的序列，它對 PolII 有更強的吸引力。除了 promoter 之外，enhancer 相較於 gene body 的距離就更不確定，可能是 10kb 或 100kb 之外，但它在立體的距離可能非常近，本身也可以 recruit TF 然後增加 Pol affinity。這一切可以用抑制、競爭的角度來想產生負向的調控。&lt;/p&gt;
&lt;p&gt;Enhancer 的影響力沒有方向性，即上下游的 gene 都會受同個 enhancer 調控。於是有所謂 TAD 的概念，它會讓 chromosome 形成一個 loop 侷限這樣立體空間上下游的互動，使得只在同個 TAD 的 REs 和 gene 能互相作用。
這樣的觀念可以進一步推廣到 3D genome 上，考慮不同 chromosome 間的互動。TAD 的邊界由某些 motif（例如 CTCF）決定，但究竟 TAD 是如果建立與調控，機制尚未明朗。&lt;/p&gt;
&lt;p&gt;在 non-coding 複雜的交互作用的另一面，代表了每個交互作用很可能僅改變了基因表達的程度，而不是大幅度的開關。但這也代表他們對生物體不一定有很強的影響，所以有變化並一定代表它有功能。不過，不同的 cell type 倒可以用透過 TF 有無來調控一系列的 gene，而不是一味增加 gene 數量。因此，在很多情況下，了解 non-coding variants 造成的影響是很有趣的。&lt;/p&gt;
&lt;h3 id="endophenotypes"&gt;Endophenotypes&lt;/h3&gt;
&lt;p&gt;我們要如何看 non-coding variants 呢？首先要了解從 genotype 到 phenotype 其實中間包含了很多層級：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DNA (genotype) →  RNA →  Proteins →  Metabolites →  Phenotype
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;中間的每個步驟都可能影響，或不會傳遞影響至下個階段。但我們在 DNA 和 RNA level 有非常好的工具─定序─可以同時看 genome wide 非常多基因或區域。於是在大多數的情況我們都只有看到 endophenotypes，要務必僅記在心這和真正的 phenotype 是有所差異的。&lt;/p&gt;
&lt;h2 id="eqtl"&gt;eQTL&lt;/h2&gt;
&lt;p&gt;eQTL 即是一種 endophenotype。QTL (quantitative trait loci) 意即某個 chromosome region 可以關連至一些量化數值的變動（即 locations that map to some quantitative measures），而 eQTL 即為 expresion QTL，關心某段區域的 variants 影響 gene expression。&lt;/p&gt;
&lt;p&gt;過往常見的 eQTL study 有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkage study&lt;/li&gt;
&lt;li&gt;Family tree&lt;/li&gt;
&lt;li&gt;GWAS on two groups&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這都是使用數個不同人不同 sample 來看 eQTL。但這些對於 non-coding variant 來說變因太多，為何不從個人、單一 sample 著手，即 allele imballance？然而單一 sample 就會牽扯到 eQTL 本身的問題，即它很難進一步從某個區域縮小到是哪個 variant 或哪幾個 variants 為決定性因子 (causal vairants)。&lt;/p&gt;
&lt;h2 id="mpra"&gt;MPRA&lt;/h2&gt;
&lt;p&gt;於是我們可以想辦法設計實驗來進一步解釋 eQTL。實驗可以從兩個方向來設計：necessary 和 sufficient。Necessity 可以透過 CRISPR 設計一系列的 tiled gRNAs 把某個 eQTL 逐步刪掉。平行化這個實驗，可以透過 growth selection 和 single cell sequencing 讀出是哪些 gRNAs 最有影響力。&lt;/p&gt;
&lt;p&gt;在 sufficiency 方面，我們可以設計 reporter assay 來回答這問題：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-----------------------------[weak promoter]--[GFP]--
---[cis RE, CRE]-------------[weak promoter]--[GFP]--
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;reporter assay 可以用個 plasmid 放到 target cell，但要怎麼平行化，同時看很多 genes 呢？這時候就是 MPRA (Massively Parallel Reporter Assay) 表現的時候了。我們可以用 DNA synthesis 把該 &lt;em&gt;cis&lt;/em&gt;-regulatory element (CRE) 和 barcode 做出來，可以建立一個 CRE library，用 RNA-seq 就可以同時看到不同 CRE 所造成的 gene expression change。當然細節有像 normalization DNA amount 和 barcode efficiency，但我們可以用 MPRA 來分析 CRE。&lt;/p&gt;
&lt;p&gt;這裡提到的 CRE-seq 有什麼缺陷呢？它是 Plasmid based，沒有 histone modification，有 copy number 問題；再來他的 genome context 也只有區域性（像 TAD 就沒有考慮）。於是接下來如何改善他，就是目前 Barak Lab 研究最新動態。&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;我覺得從這個角度，把很多觀念用系統的角度整合，並且提出新的實驗與模型，非常有趣。像要怎麼 model enhancer 和 TFs 的交互作用，都是很有趣的題目。他的課非常有啟發性，很有意思。&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:3D genome"&gt;
&lt;p&gt;關於 3D genome 就要提一下這篇論文：&lt;br&gt;
Adrian and Suhas &lt;em&gt;el al.&lt;/em&gt;, &lt;a href="http://www.pnas.org/content/112/47/E6456.abstract"&gt;&lt;em&gt;Chromatin extrusion explains key features of loop and domain formation in wild-type and engineered genomes&lt;/em&gt;&lt;/a&gt;, PNAS, 2015.&lt;br&gt;
裡面用碎形 (fractal globule) 去解釋 CTCF 形成 TADs 造成怎麼樣的染色體摺疊，並如何透過這樣的摺疊產生 long distance interaction，因為可能在立體空間他們是接近的。模型用來解釋 Hi-C 數據。這篇論文使用數學之抽象和複雜，甚至請丘成桐來當 reviewer。&amp;#160;&lt;a class="footnote-backref" href="#fnref:3D genome" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:ENCODE paper"&gt;
&lt;p&gt;針對 ENCODE 所謂 80% genome are functional 非常有名的戰文：&lt;br&gt;Dan &lt;em&gt;et al.&lt;/em&gt;, &lt;a href="https://academic.oup.com/gbe/article-lookup/doi/10.1093/gbe/evt028"&gt;&lt;em&gt;On the Immortality of Television Sets: “Function” in the Human Genome According to the Evolution-Free Gospel of ENCODE&lt;/em&gt;&lt;/a&gt;, Genome Biol Evol, 2013.&amp;#160;&lt;a class="footnote-backref" href="#fnref:ENCODE paper" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="zh"></category><category term="bio"></category></entry></feed>