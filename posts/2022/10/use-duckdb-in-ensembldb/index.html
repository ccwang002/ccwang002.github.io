<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
	<meta http-equiv="content-type" content="text/html; charset=UTF-8;charset=utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="author" content="Liang-Bo Wang" />
<meta name="description" content="I have been using ensembldb to query genome annotations locally, which stores the Ensembl annotations in a offline SQLite database. By replacing the database engine with DuckDB, genome-wide queries are faster with small impact on gene specific queries (depending on the usage). DuckDB databaseâ€™s file size is also smaller, and it can be even smaller by offloading the tables to external Parquet files." />
<meta name="keywords" content="en, r, python, ensembldb, sqlite, duckdb">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.95em%22 font-size=%2295%22>ðŸ“–</text></svg>">
	<title>Use DuckDB in ensembldb to query Ensembl's genome annotations</title>

	<!-- og -->
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="Liang-Bo Wang's Blog"/>
<meta property="og:title" content="Use DuckDB in ensembldb to query Ensembl's genome annotations"/>
<meta property="og:description" content="I have been using ensembldb to query genome annotations locally, which stores the Ensembl annotations in a offline SQLite database. By replacing the database engine with DuckDB, genome-wide queries are faster with small impact on gene specific queries (depending on the usage). DuckDB databaseâ€™s file size is also smaller, and it can be even smaller by offloading the tables to external Parquet files."/>
<meta property="og:locale" content=""/>
<meta property="og:url" content="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2022-10-05 00:00:00-05:00"/>
<meta property="article:modified_time" content="2022-10-27 13:49:17.529379-05:00"/>
<meta property="article:author" content="https://blog.liang2.tw/author/liang-bo-wang.html">
<meta property="article:section" content="Bioinfo"/>
<meta property="article:tag" content="en"/>
<meta property="article:tag" content="r"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="ensembldb"/>
<meta property="article:tag" content="sqlite"/>
<meta property="article:tag" content="duckdb"/>
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@ccwang002">
<meta name="twitter:title" content="Use DuckDB in ensembldb to query Ensembl's genome annotations">
<meta name="twitter:description" content="I have been using ensembldb to query genome annotations locally, which stores the Ensembl annotations in a offline SQLite database. By replacing the database engine with DuckDB, genome-wide queries are faster with small impact on gene specific queries (depending on the usage). DuckDB databaseâ€™s file size is also smaller, and it can be even smaller by offloading the tables to external Parquet files.">

	<!-- Feeds -->
	<link href="https://blog.liang2.tw/feeds/all.atom.xml"
		type="application/atom+xml" rel="alternate" title="Liang-Bo Wang's Blog" />

	<link rel="stylesheet" type="text/css" href="https://blog.liang2.tw/theme/styles/styles.css" />
	<link rel="stylesheet" href="https://blog.liang2.tw/theme/styles/pygments/light.css" type="text/css" media="(prefers-color-scheme: light)" id="pyg-light" />
    <link rel="stylesheet" href="https://blog.liang2.tw/theme/styles/pygments/dark-monokai.css" type="text/css" media="(prefers-color-scheme: dark)" id="pyg-dark" />
	<link rel="stylesheet" type="text/css" href="https://blog.liang2.tw/theme/katex-0.16.2/katex.min.css">
</head>

<body class="post-template no-js">
	<script>document.body.classList.remove('no-js');</script>
<svg
	xmlns="http://www.w3.org/2000/svg"
	style="display: none;"
>
	<symbol id="svg-sun-half" viewBox="0 0 24 24" pointer-events="all">
		<title>Following system color scheme</title>
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"
			stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
			<circle cx="12" cy="12" r="9"></circle>
			<path d="M12 3v18m0-12l4.65-4.65M12 14.3l7.37-7.37M12 19.6l8.85-8.85"></path>
		</svg>
	</symbol>
	<symbol id="svg-moon" viewBox="0 0 24 24" pointer-events="all">
		<title>Selected dark color scheme</title>
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"
			stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
			<path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
			<path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
		</svg>
	</symbol>
	<symbol id="svg-sun" viewBox="0 0 24 24" pointer-events="all">
		<title>Selected light color scheme</title>
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none"
			stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
			<circle cx="12" cy="12" r="5"></circle>
			<line x1="12" y1="1" x2="12" y2="3"></line>
			<line x1="12" y1="21" x2="12" y2="23"></line>
			<line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
			<line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
			<line x1="1" y1="12" x2="3" y2="12"></line>
			<line x1="21" y1="12" x2="23" y2="12"></line>
			<line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
			<line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
		</svg>
	</symbol>
</svg>

<script>
	document.documentElement.dataset.color_scheme = localStorage.getItem("color_scheme") || "auto"
</script>
	<header class="site-head">
		<h1 class="blog-title"><a href="https://blog.liang2.tw">Liang-Bo Wang's Blog</a></h1>
		<h2 class="blog-description">
			<a href="https://blog.liang2.tw/about-me/">About</a> |
			<a href="https://blog.liang2.tw/talks/">Talks</a> |
			<a href="https://blog.liang2.tw/archives.html">Archives</a> |
			<button id="color-scheme-cycler" onClick="setColorScheme(nextColorScheme())">
                <svg aria-hidden="true" class="color-scheme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
                <svg aria-hidden="true" class="color-scheme-icon-when-dark"><use href="#svg-moon"></use></svg>
                <svg aria-hidden="true" class="color-scheme-icon-when-light"><use href="#svg-sun"></use></svg>
                <span class="sr-only">Toggle light / dark / auto color theme</span>
            </button>
		</h2>
	</header>
	<main id="content" class="content" role="main">
<article class="post">
    <span class="post-meta">
        <time datetime="2022-10-05">Oct 05, 2022</time>
        in <a href="https://blog.liang2.tw/category/bioinfo/">Bioinfo</a>
    </span>
    <h1 class="post-title">
        <a href="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/" rel="bookmark" title="Permalink to Use DuckDB in ensembldb to query Ensembl's genome annotations">Use DuckDB in ensembldb to query Ensembl's genome annotations</a>
    </h1>
    <span class="post-meta">
                <a href="https://blog.liang2.tw/tag/en/">en</a>
                <a href="https://blog.liang2.tw/tag/r/">r</a>
                <a href="https://blog.liang2.tw/tag/python/">python</a>
                <a href="https://blog.liang2.tw/tag/ensembldb/">ensembldb</a>
                <a href="https://blog.liang2.tw/tag/sqlite/">sqlite</a>
                <a href="https://blog.liang2.tw/tag/duckdb/">duckdb</a>
    </span>
    <section class="post-content"><!-- cSpell:words sexchrom OLAP Hsapiens pyarrow ensdb zstandard zstd -->

<p>To query genome annotations locally, <a href="https://bioconductor.org/packages/release/bioc/html/ensembldb.html">ensembldb</a> has been my go-to approach.
While I&rsquo;ve already said many good things about this R package (<a href="https://blog.liang2.tw/posts/2016/05/biocondutor-ensembl-reference/">1</a>, <a href="https://blog.liang2.tw/posts/2017/11/use-ensdb-database-in-python/">2</a>, <a href="https://blog.liang2.tw/posts/2019/01/build-ensdb-from-local-mysql/">3</a>), here&rsquo;s a summary of my favorite features:</p>
<ol>
<li>I can use the same Ensembl version throughout my project (as a SQLite database)</li>
<li>I can query the genome-wide annotations and their locations easily and offline</li>
<li>Nice integration to R&rsquo;s ecosystem that I can easily combine the extracted annotations with my data and other annotations using <a href="https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html">GenomicRanges</a> and <a href="https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html">SummarizedExperiment</a></li>
<li>Language agnostic to use its database (say, <a href="https://blog.liang2.tw/posts/2017/11/use-ensdb-database-in-python/">I can query the same db in Python</a>)</li>
</ol>
<p>Since <a href="https://duckdb.org/">DuckDB</a> is designed for analytical query workloads (aka <a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLAP</a>), I decided to convert ensembldb&rsquo;s SQLite database to DuckDB and try it in some of my common analysis scenarios.
DuckDB has a similar look-and-feel to SQLite.
Also, it uses a columnar storage and supports query into external <a href="https://parquet.apache.org/">Apache Parquet</a> and <a href="https://arrow.apache.org/">Apache Arrow</a> tables.
I tried out some of these user-friendly features in this exercise.</p>
<div class="toc">
<ul>
<li><a href="#convert-ensembldbs-database-to-duckdb-through-parquet">Convert ensembldb&rsquo;s database to DuckDB through Parquet</a><ul>
<li><a href="#load-parquet-tables-to-duckdb">Load Parquet tables to DuckDB</a></li>
<li><a href="#database-file-size-comparison">Database file size comparison</a></li>
</ul>
</li>
<li><a href="#use-duckdb-in-ensembldb">Use DuckDB in ensembldb</a></li>
<li><a href="#benchmark-the-databases">Benchmark the databases</a><ul>
<li><a href="#genome-wide-annotation-query">Genome-wide annotation query</a></li>
<li><a href="#another-genome-wide-annotation-query">Another genome-wide annotation query</a></li>
<li><a href="#gene-specific-lookup">Gene-specific lookup</a></li>
</ul>
</li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>
<h2 id="convert-ensembldbs-database-to-duckdb-through-parquet">Convert ensembldb&rsquo;s database to DuckDB through Parquet</h2>
<p>The first step is to convert ensembldb&rsquo;s SQLite database to DuckDB<sup id="fnref:sqlite-to-duckdb"><a class="footnote-ref" href="#fn:sqlite-to-duckdb">1</a></sup>.
I decided to export the SQLite tables as individual Parquet files, and then reload them back to DuckDB.
So we could also test the DuckDB&rsquo;s ability to query external parquet files directly.</p>
<p>For this exercise, I used the <a href="https://www.ensembl.org/Homo_sapiens/">latest Ensembl release</a> (v107).
We can download the corresponding SQLite database from <a href="https://annotationhub.bioconductor.org/package2/AHEnsDbs">AnnotationHub&rsquo;s web interface</a> (its object ID is <a href="https://annotationhub.bioconductor.org/ahid/AH104864">AH104864</a>):</p>
<div class="highlight"><pre><span></span><code>curl -Lo EnsDb.Hsapiens.v107.sqlite <span class="se">\</span>
    https://annotationhub.bioconductor.org/fetch/111610
</code></pre></div>

<p>We use <a href="https://www.sqlalchemy.org/">SQLAlchemy</a> to fetch the schema of all the tables:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sqlalchemy</span> <span class="kn">import</span> <span class="n">MetaData</span><span class="p">,</span> <span class="n">create_engine</span>

<span class="n">engine</span> <span class="o">=</span> <span class="n">create_engine</span><span class="p">(</span><span class="s1">&#39;sqlite:///EnsDb.Hsapiens.v107.sqlite&#39;</span><span class="p">)</span>
<span class="n">metadata</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">()</span>
<span class="n">metadata</span><span class="o">.</span><span class="n">reflect</span><span class="p">(</span><span class="n">bind</span><span class="o">=</span><span class="n">engine</span><span class="p">)</span>
</code></pre></div>

<p>We can then list all the tables and their column data types:</p>
<div class="highlight"><pre><span></span><code><span class="n">db_tables</span> <span class="o">=</span> <span class="n">metadata</span><span class="o">.</span><span class="n">sorted_tables</span>
<span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">db_tables</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">table</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">: &#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s1">)&#39;</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
<span class="c1"># chromosome: seq_name (TEXT), seq_length (INTEGER), is_circular (INTEGER)</span>
<span class="c1"># gene: gene_id (TEXT), gene_name (TEXT), gene_biotype (TEXT),</span>
<span class="c1">#   gene_seq_start (INTEGER), gene_seq_end (INTEGER),</span>
<span class="c1">#   seq_name (TEXT), seq_strand (INTEGER),...</span>
<span class="c1"># ...</span>
</code></pre></div>

<p>With the correct data type mapping, we can export all the tables as Parquet by <a href="https://pandas.pydata.org/">pandas</a> and <a href="https://arrow.apache.org/docs/python/index.html">PyArrow</a>.
Since there are quite many text columns, I also used <a href="https://facebook.github.io/zstd/">zstandard</a> to compress the Parquet (with a higher compression level):</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="nn">pa</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pyarrow.parquet</span> <span class="k">as</span> <span class="nn">pq</span>

<span class="n">sqlite_to_pyarrow_type_mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;TEXT&#39;</span><span class="p">:</span> <span class="n">pa</span><span class="o">.</span><span class="n">string</span><span class="p">(),</span>
    <span class="s1">&#39;INTEGER&#39;</span><span class="p">:</span> <span class="n">pa</span><span class="o">.</span><span class="n">int64</span><span class="p">(),</span>
    <span class="s1">&#39;REAL&#39;</span><span class="p">:</span> <span class="n">pa</span><span class="o">.</span><span class="n">float64</span><span class="p">(),</span>
<span class="p">}</span>

<span class="c1"># Read each SQLite table as a Arrow table</span>
<span class="n">arrow_tables</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">with</span> <span class="n">engine</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">metadata</span><span class="o">.</span><span class="n">sorted_tables</span><span class="p">:</span>
        <span class="c1"># Construct the corresponding pyarrow schema</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">schema</span><span class="p">([</span>
            <span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">sqlite_to_pyarrow_type_mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">type</span><span class="p">)])</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">table</span><span class="o">.</span><span class="n">columns</span>
        <span class="p">])</span>
        <span class="n">arrow_tables</span><span class="p">[</span><span class="n">table</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">Table</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">read_sql_table</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">conn</span><span class="p">,</span> <span class="n">coerce_float</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span>
            <span class="n">preserve_index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

<span class="c1"># Write each Arrow table to a zstd compressed Parquet</span>
<span class="k">for</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">arrow_tables</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">pq</span><span class="o">.</span><span class="n">write_table</span><span class="p">(</span>
        <span class="n">table</span><span class="p">,</span>
        <span class="sa">f</span><span class="s1">&#39;ensdb_v107/</span><span class="si">{</span><span class="n">table_name</span><span class="si">}</span><span class="s1">.parquet&#39;</span><span class="p">,</span>
        <span class="n">compression</span> <span class="o">=</span> <span class="s1">&#39;zstd&#39;</span><span class="p">,</span>
        <span class="n">compression_level</span> <span class="o">=</span> <span class="mi">9</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>

<h3 id="load-parquet-tables-to-duckdb">Load Parquet tables to DuckDB</h3>
<p>Finally, we can load the exported Parquet tables to DuckDB.
Here I tested a few approaches:</p>
<ol>
<li>Create views to the external Parquet files (no content loaded to the db)</li>
<li>Load the full content</li>
<li>Load the full content and index the tables (same as the original SQLite db)</li>
</ol>
<p>Since DuckDB has native support for Parquet files, the syntax is straightforward:</p>
<div class="highlight"><pre><span></span><code><span class="c1">-- Install and activate the extension</span>
<span class="n">INSTALL</span><span class="w"> </span><span class="n">parquet</span><span class="p">;</span><span class="w"> </span><span class="k">LOAD</span><span class="w"> </span><span class="n">parquet</span><span class="p">;</span><span class="w"></span>

<span class="c1">-- To create views to external Parquet</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">VIEW</span><span class="w"> </span><span class="o">&lt;</span><span class="k">table</span><span class="o">&gt;</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="s1">&#39;./ensdb_v107/&lt;table&gt;.parquet&#39;</span><span class="p">;</span><span class="w"></span>
<span class="p">...</span><span class="w"></span>

<span class="c1">-- To load the full content from external Parquet</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="o">&lt;</span><span class="k">table</span><span class="o">&gt;</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="s1">&#39;./ensdb_v107/&lt;table&gt;.parquet&#39;</span><span class="p">;</span><span class="w"></span>
<span class="p">...</span><span class="w"></span>

<span class="c1">-- To index the table (use .schema to get the original index definitions)</span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">UNIQUE</span><span class="w"> </span><span class="k">INDEX</span><span class="w"> </span><span class="n">gene_gene_id_idx</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">gene</span><span class="w"> </span><span class="p">(</span><span class="n">gene_id</span><span class="p">);</span><span class="w"></span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">INDEX</span><span class="w"> </span><span class="n">gene_gene_name_idx</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">gene</span><span class="w"> </span><span class="p">(</span><span class="n">gene_name</span><span class="p">);</span><span class="w"></span>
<span class="k">CREATE</span><span class="w"> </span><span class="k">INDEX</span><span class="w"> </span><span class="n">gene_seq_name_idx</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">gene</span><span class="w"> </span><span class="p">(</span><span class="n">seq_name</span><span class="p">);</span><span class="w"></span>
<span class="p">...</span><span class="w"></span>
</code></pre></div>

<p>Note that I didn&rsquo;t try to &ldquo;optimize&rdquo; the table indices for my queries.
I simply mirrored the same index definition from the original SQLite database.</p>
<p>DuckDB&rsquo;s commandline interface works like SQLite.
And it keeps the database in a single file too.
The full conversion including the Parquet step took about 10 seconds to complete.</p>
<div class="highlight"><pre><span></span><code>duckdb -echo ensdb_v107.duckdb &lt; create_duckdb.sql
duckdb -readonly ensdb_v107.duckdb
</code></pre></div>

<h3 id="database-file-size-comparison">Database file size comparison</h3>
<p>Here shows the file size of the databases created with different settings:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Database</th>
<th style="text-align: right;">File size</th>
<th style="text-align: right;">(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SQLite no indexed</td>
<td style="text-align: right;">243MB</td>
<td style="text-align: right;">57.9</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SQLite (original)</strong></td>
<td style="text-align: right;"><strong>420MB</strong></td>
<td style="text-align: right;"><strong>100.0</strong></td>
</tr>
<tr>
<td style="text-align: left;">DuckDB with external Parquets</td>
<td style="text-align: right;">37.6MB</td>
<td style="text-align: right;">9.0</td>
</tr>
<tr>
<td style="text-align: left;">DuckDB</td>
<td style="text-align: right;">169MB</td>
<td style="text-align: right;">40.2</td>
</tr>
<tr>
<td style="text-align: left;">DuckDB indexed</td>
<td style="text-align: right;">528MB</td>
<td style="text-align: right;">125.7</td>
</tr>
</tbody>
</table>
<p>DuckDB with external Parquets yields the smallest file (~9% of the original size).
It&rsquo;s probably due to a lot of text columns in the database, and zstd compression works really well for the plain text.
This approach could make the ensembldb database more portable.
Say, it&rsquo;s possible to commit it directly into the analysis project&rsquo;s GitHub repo.</p>
<p>By loading the actual data into DuckDB (without indices), the file grows considerably due to no compression.
Though it is slightly smaller than its SQLite counterpart.
I wonder if this is due to the columnar storage being more space efficient than row storage.
After indexing the DuckDB database, it surprisingly grows to be much larger than SQLite.
I don&rsquo;t know DuckDB&rsquo;s indexing methods enough to understand what happened here.
Since DuckDB is still actively developing its indexing algorithm, I suppose this could be optimized in the future.</p>
<p>Now we have the databases ready.
Let&rsquo;s see how they perform.</p>
<!-- cSpell:words dbdir mircrobenchmark noidx EGFR -->

<h2 id="use-duckdb-in-ensembldb">Use DuckDB in ensembldb</h2>
<p>It&rsquo;s painless to tell ensembldb to use DuckDB instead.
<a href="https://duckdb.org/docs/api/r">DuckDB&rsquo;s R client</a> already implements R&rsquo;s DBI interface, and ensembldb <a href="https://jorainer.github.io/ensembldb/reference/EnsDb.html">accepts a DBI connection</a> to create a EnsDb object.
So we already have everything we need:</p>
<div class="highlight"><pre><span></span><code><span class="nf">library</span><span class="p">(</span><span class="n">duckdb</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ensembldb</span><span class="p">)</span>

<span class="n">edb_sqlite</span> <span class="o">=</span> <span class="nf">EnsDb</span><span class="p">(</span><span class="s">&#39;EnsDb.Hsapiens.v107.sqlite&#39;</span><span class="p">)</span>

<span class="n">conn</span> <span class="o">=</span> <span class="nf">dbConnect</span><span class="p">(</span><span class="nf">duckdb</span><span class="p">(),</span> <span class="n">dbdir</span> <span class="o">=</span> <span class="s">&quot;ensdb_v107.duckdb&quot;</span><span class="p">,</span> <span class="n">read_only</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
<span class="n">edb_duckdb</span> <span class="o">=</span> <span class="nf">EnsDb</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>
<span class="nf">dbDisconnect</span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">shutdown</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>  <span class="c1"># disconnect after usage</span>
</code></pre></div>

<p>All the downstream usage of ensembldb is the same from here.</p>
<h2 id="benchmark-the-databases">Benchmark the databases</h2>
<p>Now we have the original SQLite database and three DuckDB databases constructed with various settings ready to use in ensembldb.
Here I tested two scenarios: a genome-wide annotation query and a gene-specific lookup.</p>
<p>To make the query more realistic and complicated, I also applied a filter to all queries to select annotations only from the canonical chromosomes and remove all LRG genes:</p>
<div class="highlight"><pre><span></span><code><span class="n">standard_filter</span> <span class="o">=</span> <span class="nf">AnnotationFilter</span><span class="p">(</span>
    <span class="o">~</span> <span class="n">seq_name</span> <span class="o">%in%</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">22</span><span class="p">,</span> <span class="s">&#39;X&#39;</span><span class="p">,</span> <span class="s">&#39;Y&#39;</span><span class="p">,</span> <span class="s">&#39;MT&#39;</span><span class="p">)</span> <span class="o">&amp;</span>
        <span class="n">gene_biotype</span> <span class="o">!=</span> <span class="s">&#39;LRG_gene&#39;</span>
<span class="p">)</span>
</code></pre></div>

<p>I use <a href="https://cran.r-project.org/web/packages/microbenchmark/index.html">microbenchmark</a> to benchmark the same query from different databases.
It works like this:</p>
<div class="highlight"><pre><span></span><code><span class="n">mbm</span> <span class="o">=</span> <span class="nf">microbenchmark</span><span class="p">(</span>
    <span class="s">&quot;sqlite_noidx&quot;</span> <span class="o">=</span> <span class="p">{</span> <span class="o">&lt;</span><span class="n">some</span> <span class="n">query</span><span class="o">&gt;</span> <span class="p">},</span>
    <span class="s">&quot;sqlite&quot;</span> <span class="o">=</span> <span class="p">{</span> <span class="kc">...</span> <span class="p">},</span>
    <span class="s">&quot;duckdb_parquet&quot;</span> <span class="o">=</span> <span class="p">{</span> <span class="kc">...</span> <span class="p">},</span>
    <span class="s">&quot;duckdb&quot;</span> <span class="o">=</span> <span class="p">{</span> <span class="kc">...</span> <span class="p">},</span>
    <span class="s">&quot;duckdb_idx&quot;</span> <span class="o">=</span> <span class="p">{</span> <span class="kc">...</span> <span class="p">},</span>
    <span class="n">times</span> <span class="o">=</span> <span class="m">20</span>  <span class="c1"># 50 times for faster queries</span>
<span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">mbm</span><span class="p">)</span>
</code></pre></div>

<h3 id="genome-wide-annotation-query">Genome-wide annotation query</h3>
<p>The first genome-wide query finds the 5&rsquo;UTR genomic ranges of all the transcripts.
This is one of the most computationally intensive built-in queries I know, involving some genomic range arithmics and querying over multiple tables.</p>
<div class="highlight"><pre><span></span><code><span class="n">five_utr_per_tx</span> <span class="o">=</span> <span class="nf">fiveUTRsByTranscript</span><span class="p">(</span><span class="n">edb</span><span class="p">,</span> <span class="n">filter</span> <span class="o">=</span> <span class="n">standard_filter</span><span class="p">)</span>
<span class="n">five_utr_per_tx</span> <span class="o">|&gt;</span> <span class="nf">head</span><span class="p">()</span>
<span class="c1">## GRangesList object of length 6:</span>
<span class="c1">## $ENST00000000442</span>
<span class="c1">## GRanges object with 2 ranges and 4 metadata columns:</span>
<span class="c1">##       seqnames            ranges strand |   gene_biotype    seq_name</span>
<span class="c1">##          &lt;Rle&gt;         &lt;IRanges&gt;  &lt;Rle&gt; |    &lt;character&gt; &lt;character&gt;</span>
<span class="c1">##   [1]       11 64305524-64305736      + | protein_coding          11</span>
<span class="c1">##   [2]       11 64307168-64307179      + | protein_coding          11</span>
<span class="c1">##               exon_id exon_rank</span>
<span class="c1">##           &lt;character&gt; &lt;integer&gt;</span>
<span class="c1">##   [1] ENSE00001884684         1</span>
<span class="c1">##   [2] ENSE00001195360         2</span>
<span class="c1">##   -------</span>
<span class="c1">##   seqinfo: 25 sequences (1 circular) from GRCh38 genome</span>
<span class="c1">##</span>
<span class="c1">## ...</span>
</code></pre></div>

<p>Here is the microbenchmark results by running the same query in all databases:</p>
<figure class="invert-in-dark-mode">
    <img src="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/pics/benchmark_genomewide_5utr_by_tx.png">
    <figcaption>Benchmark results of extracting genome-wide 5'UTR locations per transcript.</figcaption>
</figure>

<p>There is a huge performance increase for all DuckDB databases, since this query pretty much scans over the full table.
Overall, DuckDB runs 3+ times faster than SQLite.</p>
<p>In many cases, there are always a few runs in each database that take significantly more time.
This trend is quite consistent as I re-run the benchmarks multiple times.
While I haven&rsquo;t investigated these outliers, I think this is due to the first run(s) being un-cached.
Surprisingly, DuckDB with indices run much slower than that without indices (especially the first run).
Though the index might be useless in sequential scans, I guess the slowdown could be due to the bigger file (longer to cache) or the query planner accidentally traversing over indices.</p>
<h3 id="another-genome-wide-annotation-query">Another genome-wide annotation query</h3>
<p>The other genome-wide query finds the transcripts of all the genes.</p>
<div class="highlight"><pre><span></span><code><span class="n">tx_per_gene</span> <span class="o">=</span> <span class="nf">transcriptsBy</span><span class="p">(</span><span class="n">edb</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="s">&quot;gene&quot;</span><span class="p">,</span> <span class="n">filter</span> <span class="o">=</span> <span class="n">standard_filter</span><span class="p">)</span>
<span class="n">tx_per_gene</span> <span class="o">|&gt;</span> <span class="nf">head</span><span class="p">()</span>
<span class="c1">## GRangesList object of length 6:</span>
<span class="c1">## $ENSG00000000003</span>
<span class="c1">## GRanges object with 5 ranges and 12 metadata columns:</span>
<span class="c1">##       seqnames              ranges strand |           tx_id</span>
<span class="c1">##          &lt;Rle&gt;           &lt;IRanges&gt;  &lt;Rle&gt; |     &lt;character&gt;</span>
<span class="c1">##   [1]        X 100633442-100639991      - | ENST00000494424</span>
<span class="c1">##   [2]        X 100627109-100637104      - | ENST00000612152</span>
<span class="c1">##   [3]        X 100632063-100637104      - | ENST00000614008</span>
<span class="c1">##   [4]        X 100627108-100636806      - | ENST00000373020</span>
<span class="c1">##   [5]        X 100632541-100636689      - | ENST00000496771</span>
<span class="c1">##                 tx_biotype tx_cds_seq_start tx_cds_seq_end         gene_id</span>
<span class="c1">##                &lt;character&gt;        &lt;integer&gt;      &lt;integer&gt;     &lt;character&gt;</span>
<span class="c1">##   [1] processed_transcript             &lt;NA&gt;           &lt;NA&gt; ENSG00000000003</span>
<span class="c1">##   [2]       protein_coding        100630798      100635569 ENSG00000000003</span>
<span class="c1">##   [3]       protein_coding        100632063      100635569 ENSG00000000003</span>
<span class="c1">##   [4]       protein_coding        100630798      100636694 ENSG00000000003</span>
<span class="c1">##   [5] processed_transcript             &lt;NA&gt;           &lt;NA&gt; ENSG00000000003</span>
<span class="c1">## ...</span>
</code></pre></div>

<p>Similarly, here are the benchmark results:</p>
<figure class="invert-in-dark-mode">
    <img src="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/pics/benchmark_genomewide_tx_by_gene.png">
    <figcaption>Benchmark of extracting genome-wide gene isoforms.</figcaption>
</figure>

<p>This query tells more or less the same story with only a notable difference.
In this case, fully loaded DuckDB with and without indices share the same performance.
Interestingly, all the DuckDB runtimes are in a bimodal distribution.
I don&rsquo;t know why.</p>
<h3 id="gene-specific-lookup">Gene-specific lookup</h3>
<p>My another main scenario is to look up the annotations of a specific gene.
Let&rsquo;s simulate this kind of queries by retrieving all the transcripts of a gene &ldquo;EGFR&rdquo;:</p>
<div class="highlight"><pre><span></span><code><span class="n">egfr_tx</span> <span class="o">=</span> <span class="nf">transcripts</span><span class="p">(</span><span class="n">edb</span><span class="p">,</span> <span class="n">filter</span> <span class="o">=</span> <span class="nf">AnnotationFilter</span><span class="p">(</span><span class="o">~</span> <span class="n">gene_name</span> <span class="o">==</span> <span class="s">&#39;EGFR&#39;</span><span class="p">))</span>
<span class="n">egfr_tx</span>
<span class="c1">## GRanges object with 14 ranges and 12 metadata columns:</span>
<span class="c1">##                   seqnames            ranges strand |           tx_id</span>
<span class="c1">##                      &lt;Rle&gt;         &lt;IRanges&gt;  &lt;Rle&gt; |     &lt;character&gt;</span>
<span class="c1">##   ENST00000344576        7 55019017-55171037      + | ENST00000344576</span>
<span class="c1">##   ENST00000275493        7 55019017-55211628      + | ENST00000275493</span>
<span class="c1">##   ENST00000455089        7 55019021-55203076      + | ENST00000455089</span>
<span class="c1">##   ENST00000342916        7 55019032-55168635      + | ENST00000342916</span>
<span class="c1">##         LRG_304t1        7 55019032-55207338      + |       LRG_304t1</span>
<span class="c1">##               ...      ...               ...    ... .             ...</span>
<span class="c1">##   ENST00000450046        7 55109723-55211536      + | ENST00000450046</span>
<span class="c1">##   ENST00000700145        7 55163753-55205865      + | ENST00000700145</span>
<span class="c1">##   ENST00000485503        7 55192811-55200802      + | ENST00000485503</span>
<span class="c1">##   ENST00000700146        7 55198272-55208067      + | ENST00000700146</span>
<span class="c1">##   ENST00000700147        7 55200573-55206016      + | ENST00000700147</span>
<span class="c1">## ...</span>
</code></pre></div>

<figure class="invert-in-dark-mode">
    <img src="https://blog.liang2.tw/posts/2022/10/use-duckdb-in-ensembldb/pics/benchmark_extract_specific_gene.png">
    <figcaption>Benchmark of extracting the annotations of a specific gene.</figcaption>
</figure>

<p>SQLite with indices undoubtedly has the best performance.
Understandably, it&rsquo;s been fine tuned for this very use case (extracting a few rows using indices).
And SQLite without an index takes the most time to complete, so it&rsquo;s necessary to always index the tables.</p>
<p>The performance of all three DuckDB databases fall in between the two extremes of SQLite dbs.
Unlike SQLite, indexed DuckDB only speeds up the query a little bit (21.0ms vs 22.4ms on average).
Given the worse performance of one of the genome-wide queries above using the indexed DuckDB db,
I think it&rsquo;s optional to create indices for ensembldb&rsquo;s DuckDB dbs.</p>
<h2 id="summary">Summary</h2>
<p>Here is the overview of the benchmarking results together with the db&rsquo;s file size.
The table below displays the performance in average speed-up ratio (and the worst case ratio) over the original SQLite db (ratio the higher the better):</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Database</th>
<th style="text-align: right;">Size (%)</th>
<th style="text-align: right;">Genome I</th>
<th style="text-align: right;">Genome II</th>
<th style="text-align: right;">Gene-specific lookup</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SQLite no indexed</td>
<td style="text-align: right;">57.9</td>
<td style="text-align: right;">0.61 (0.78)</td>
<td style="text-align: right;">0.88 (1.02)</td>
<td style="text-align: right;">0.044 (0.12)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>SQLite (original)</strong></td>
<td style="text-align: right;"><strong>100.0</strong></td>
<td style="text-align: right;"><strong>1.00 (1.00)</strong></td>
<td style="text-align: right;"><strong>1.00 (1.00)</strong></td>
<td style="text-align: right;"><strong>1.00 (1.00)</strong></td>
</tr>
<tr>
<td style="text-align: left;">DuckDB w. ext. Parquets</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">3.36 (3.69)</td>
<td style="text-align: right;">4.14 (4.63)</td>
<td style="text-align: right;">0.15 (0.35)</td>
</tr>
<tr>
<td style="text-align: left;">DuckDB</td>
<td style="text-align: right;">40.2</td>
<td style="text-align: right;">4.70 (4.76)</td>
<td style="text-align: right;">6.30 (6.73)</td>
<td style="text-align: right;">0.61 (0.81)</td>
</tr>
<tr>
<td style="text-align: left;">DuckDB indexed</td>
<td style="text-align: right;">125.7</td>
<td style="text-align: right;">3.66 (1.87)</td>
<td style="text-align: right;">6.29 (6.33)</td>
<td style="text-align: right;">0.65 (1.80)</td>
</tr>
</tbody>
</table>
<p>Overall, DuckDB shows impressive performance increase for genome-wide queries.
It uses up less storage too.
While DuckDB is slower than SQLite when it comes to gene-specific lookups, since we are talking about tens of milliseconds per query, unless we are running thousands of these queries, the performance impact is minimal.
On the other hand, genome-wide queries are saving seconds per query.</p>
<p>As the benchmark results shown, we could replace the original ensembldb database with a DuckDB database by loading the tables and removing the indices.
If the user is willing to sacrifice some performance in gene-specific lookups, DuckDB with external Parquet files only uses &lt; 10% of the original disk space but it still runs faster for genome-wide queries.</p>
<p>While the default indices copied from SQLite are not very helpful, I didn&rsquo;t tune the indices to maximally speed up the gene-specific lookups.
We can probably also tune the Parquet compression ratio to find a better balance between the decompression speed and file size.
Note that DuckDB&rsquo;s file format is not stabilized yet, so the database needs to be re-created in newer DuckDB versions.</p>
<p>All in all, I think DuckDB advertises itself accurately when it comes to analytical query workloads.
It shows good performance when it queries a large portion of its content.
By having a similar interface to SQLite and clients in popular languages (R, Python, and etc),
it&rsquo;s easy to change an existing SQLite usecase to use DuckDB.
My small exercise with ensembldb has convinced me to try out DuckDB in more scenarios too.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:sqlite-to-duckdb">
<p>There is an official extension <a href="https://github.com/duckdblabs/sqlite_scanner">sqlite_scanner</a> currently under development that lets a DuckDB attach directly to a SQLite database.
So in the future, it could be much easier to convert SQLite to DuckDB.&#160;<a class="footnote-backref" href="#fnref:sqlite-to-duckdb" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div></section>
    <footer class="post-footer">
	<section
		class="utterances"
		data-repo="ccwang002/ccwang002.github.io"
		data-issue-term="pathname"
		data-label="Comment">
	</section>
	</footer>
</article>
	</main>
	<footer class="site-footer">
		<a class="subscribe"
			href="https://blog.liang2.tw/feeds/all.atom.xml"
			aria-label="Subscribe the Atom feed"
			data-tooltip="Subscribe"
		>
			<svg viewBox="0 0 24 24" width="24" height="24">
				<title>Atom feed</title>
				<circle cx="6.18" cy="17.82" r="2.18"/>
				<path d="m4 10.1v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9zm0-5.66v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56z"/>
			</svg>
		</a>
		<div class="inner">
			<section>
				Browse articles by
				<a href="https://blog.liang2.tw/tags.html">Tags</a> |
				<a href="https://blog.liang2.tw/categories.html">Categories</a>
			</section>
			<section class="contact">
				Contact me by
				<a href="https://twitter.com/lbwang2">Twitter</a> |
				<a href="https://www.facebook.com/lbwang.2">Facebook</a> |
				<a href="https://github.com/ccwang002">GitHub</a> |
				<a href="mailto:me+blog@liang2.tw">Email</a> |
				<a href="https://www.linkedin.com/in/liangbowang/">LinkedIn</a>
			</section>
			<section class="copyright">
				This work is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
			</section>
			<section class="powered-by">
				Built using <a href="https://getpelican.com/">Pelican</a>.
				Powered by the <a href="https://github.com/kura/hauntr">Hauntr</a> theme.
			</section>
		</div>
	</footer>
	<script src="https://blog.liang2.tw/theme/scripts/color_scheme.js"></script>
</body>
</html>